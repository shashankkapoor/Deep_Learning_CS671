# -*- coding: utf-8 -*-
"""Group9_Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c309RETyHOfX6SPK5PpXBeV7IQkClwkr

#### Mounting Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""#### Importing libraries"""

import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import numpy as np
import math

"""# Classification Task

#### linearly separable Data
"""

class_1 = '/content/drive/MyDrive/Deep_learning/Group09_Assignment1/Classification/LS_Group09/Class1.txt'
class_2 = '/content/drive/MyDrive/Deep_learning/Group09_Assignment1/Classification/LS_Group09/Class2.txt'
class_3 = '/content/drive/MyDrive/Deep_learning/Group09_Assignment1/Classification/LS_Group09/Class3.txt'

data_class_1 = np.loadtxt(class_1, delimiter=' ', skiprows=0, dtype=str)
data_class_2 = np.loadtxt(class_2, delimiter=' ', skiprows=0, dtype=str)
data_class_3 = np.loadtxt(class_3, delimiter=' ', skiprows=0, dtype=str)

data_class_1=np.insert(data_class_1.astype(float), 2, 1, axis=1)
data_class_2=np.insert(data_class_2.astype(float), 2, 1, axis=1)
data_class_3=np.insert(data_class_3.astype(float), 2, 1, axis=1)

train_c1,test_c1 = np.split(data_class_1,[int(0.7 * len(data_class_1))])
train_c2,test_c2 = np.split(data_class_2,[int(0.7 * len(data_class_2))])
train_c3,test_c3 = np.split(data_class_3,[int(0.7 * len(data_class_3))])

def training_perceptron(n_epoch,training_data,weights,target_training_label,learning_factor):
  Avg_error_list=np.array([])
  
  counter=0
  while counter!=n_epoch:
    Error_list=np.array([])
    for i in np.random.permutation(len(training_data)):
      predicted_value=Activation_value(training_data[i],weights)
      error=0.5*(target_training_label[i]-predicted_value)**2
      Error_list=np.append(Error_list, error)
      delta_weight=(learning_factor*(target_training_label[i]-predicted_value)*predicted_value*(1-predicted_value))*training_data[i]
      weights=np.add(weights, delta_weight)
    Average_error=np.mean(Error_list, axis = 0)  
    Avg_error_list=np.append(Avg_error_list, Average_error) 
    print("At Epoch="+str(counter)+", the average error is calculated as: "+str(Average_error))
    
    counter=counter+1
  yield Avg_error_list
  yield weights

def plot_decision_regions(X, y,weights,markers,colors, test_idx=None, resolution=0.1):

    cmap = ListedColormap(colors[:len(np.unique(y))])

    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))
    
    
    Z= np.array([xx1.ravel(), xx2.ravel()]).T
    data_class=np.insert(Z.astype(float), 2, 1, axis=1)
    predict=np.array([])
    for i in range(len(Z)): 
      predict=np.append(predict,np.dot(data_class[i],weights))

    predict=predict.reshape(xx1.shape)
    plt.contourf(xx1, xx2, predict, alpha=0.3, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], 
                    y=X[y == cl, 1],
                    alpha=0.8, 
                    c=colors[idx],
                    marker=markers[idx], 
                    label=cl, 
                    edgecolor='white')

    # # highlight test samples
    # if test_idx:
    #     # plot all samples
    #     X_test, y_test = X[test_idx, :], y[test_idx]

    #     plt.scatter(X_test[:, 0],
    #                 X_test[:, 1],
    #                 c='',
    #                 edgecolor='black',
    #                 alpha=1.0,
    #                 linewidth=1,
    #                 marker='o',
    #                 s=100, 
    #                 label='test set')

"""##### Training Class1-Class2 perceptron"""

train_c1_c2=np.concatenate((train_c1, train_c2), axis=0)
Target_label_c1_c2= np.concatenate((np.ones(len(train_c1)),np.zeros(len(train_c2))), axis=None)
weights_c1_c2=np.array([0.4,0.4,0.4]) 

n_epoch=1000
learning_factor=0.05

print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")
result=training_perceptron(n_epoch,train_c1_c2,weights_c1_c2,Target_label_c1_c2,learning_factor)

Avg_error_c1_c2=next(result)
weights_c1_c2=next(result)

"""##### Training Class1-Class3 perceptron"""

train_c1_c3=np.concatenate((train_c1, train_c3), axis=0)
Target_label_c1_c3= np.concatenate((np.ones(len(train_c1)),np.zeros(len(train_c3))), axis=None)
weights_c1_c3=np.array([0.4,0.4,0.4]) 

n_epoch=1000
learning_factor=0.05

print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")
result=training_perceptron(n_epoch,train_c1_c3,weights_c1_c3,Target_label_c1_c3,learning_factor)

Avg_error_c1_c3=next(result)
weights_c1_c3=next(result)

"""##### Training Class2-Class3 perceptron"""

train_c2_c3=np.concatenate((train_c2, train_c3), axis=0)
Target_label_c2_c3= np.concatenate((np.ones(len(train_c2)),np.zeros(len(train_c3))), axis=None)
weights_c2_c3=np.array([0.4,0.4,0.4]) 

n_epoch=1000
learning_factor=0.05

print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")
result=training_perceptron(n_epoch,train_c2_c3,weights_c2_c3,Target_label_c2_c3,learning_factor)

Avg_error_c2_c3=next(result)
weights_c2_c3=next(result)

"""##### Graph Plot linear Separable Data"""

fig, axs = plt.subplots(3,figsize=(7,5),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs[0].set_title('Class1-class2 Perceptron')
axs[0].plot(list(range(1,n_epoch+1)), Avg_error_c1_c2)
axs[0].set(xlabel='No. of Epochs.', ylabel='Average Error')

axs[1].set_title('Class1-class3 Perceptron')
axs[1].plot(list(range(1,n_epoch+1)), Avg_error_c1_c3)
axs[1].set(xlabel='No. of Epochs.', ylabel='Average Error')

axs[2].set_title('Class2-class3 Perceptron')
axs[2].plot(list(range(1,n_epoch+1)), Avg_error_c2_c3)
axs[2].set(xlabel='No. of Epochs.', ylabel='Average Error')

"""##### Class1-class2 Decision Region Plot"""

X=np.delete(train_c1_c2,2,axis=1)
y=Target_label_c1_c2
weights=weights_c1_c2
markers = ('s', 'o')       #, '^', 'v')
colors = ('green', 'blue') #, 'lightgreen') #, 'gray', 'cyan')


plot_decision_regions(X, y, weights,markers,colors,test_idx=None, resolution=0.1)

"""##### Class1-class3 Decision Region Plot"""

X=np.delete(train_c1_c3,2,axis=1)
y=Target_label_c1_c3
weights=weights_c1_c3
markers = ('s', '^')       #, '^', 'v')
colors = ('green', 'red') #, 'lightgreen') #, 'gray', 'cyan')

plot_decision_regions(X, y, weights,markers,colors,test_idx=None, resolution=0.1)

"""##### Class2-class3 Decision Region Plot"""

X=np.delete(train_c2_c3,2,axis=1)
y=Target_label_c2_c3
weights=weights_c2_c3
markers = ('o', '^')       #, '^', 'v')
colors = ('blue','red') #, 'lightgreen') #, 'gray', 'cyan')

plot_decision_regions(X, y, weights,markers,colors,test_idx=None, resolution=0.1)

"""##### Class1-class2-class3 Decision Region Plot"""

X=np.delete(train_c1_c2,2,axis=1)
y=Target_label_c1_c2_c3
weights=weights_c1_c2_c3
markers = ('s', 'o')       #, '^', 'v')
colors = ('green', 'blue') #, 'lightgreen') #, 'gray', 'cyan')

plot_decision_regions(X, y, weights ,test_idx=None, resolution=0.5)

def Activation_value(x,w):
  z=np.dot(x,w)
  return Output_signal(z)

def Output_signal(z):
   return (1/(1+np.exp(-z)))

"""#### Non-linearly separable data"""

path_nls ='/content/drive/MyDrive/Deep_learning/Group09_Assignment1/Classification/NLS_Group09.txt'

Data_nls= np.loadtxt(path_nls, delimiter=' ', skiprows=1, dtype=str)

data_class1_nls,data_class2_nls, data_class3_nls=np.split(Data_nls,[500, 1000])

data_class1_nls=np.delete(data_class1_nls,2,axis=1)
data_class2_nls=np.delete(data_class2_nls,2,axis=1)
data_class3_nls=np.delete(data_class3_nls,2,axis=1)

data_class1_nls=np.insert(data_class1_nls.astype(float), 2, 1, axis=1)
data_class2_nls=np.insert(data_class2_nls.astype(float), 2, 1, axis=1)
data_class3_nls=np.insert(data_class3_nls.astype(float), 2, 1, axis=1)


train_nls_c1,test_nls_c1 = np.split(data_class1_nls,[int(0.7 * len(data_class1_nls))])
train_nls_c2,test_nls_c2 = np.split(data_class2_nls,[int(0.7 * len(data_class2_nls))])
train_nls_c3,test_nls_c3 = np.split(data_class3_nls,[int(0.7 * len(data_class3_nls))])

weights_nls_c1_c2=np.array([0.4,0.4,0.4]) # [w1,w2,w0] between the input range
weights_nls_c1_c3=np.array([0.4,0.4,0.4]) # [w1,w2,w0]
weights_nls_c2_c3=np.array([0.4,0.4,0.4]) # [w1,w2,w0]

Error_nls_c1_c2=np.array([])
Error_nls_c1_c3=np.array([])
Error_nls_c2_c3=np.array([])

Avg_error_nls_c1_c2=np.array([])
Avg_error_nls_c1_c3=np.array([])
Avg_error_nls_c2_c3=np.array([])

print("<-----------------No. of Epoch=50----------------->")
n_epoch=50
learning_factor=0.2

"""##### Training Non-linear separable Class1-Class2 perceptron """

train_nls_c1_c2=np.concatenate((train_nls_c1, train_nls_c2), axis=0)
Target_label_nls_c1_c2= np.concatenate((np.ones(len(train_nls_c1)),np.zeros(len(train_nls_c2))), axis=None)
Avg_error_nls_c1_c2=np.array([])

n_epoch=1000
learning_factor=0.01

print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")
result=training_perceptron(n_epoch,train_nls_c1_c2,weights_nls_c1_c2,Target_label_nls_c1_c2,learning_factor)

Avg_error_nls_c1_c2=next(result)
weights_nls_c1_c2=next(result)

"""##### Training Non-linear separable Class1-Class3 perceptron"""

train_nls_c1_c3=np.concatenate((train_nls_c1, train_nls_c3), axis=0)
Target_label_nls_c1_c3= np.concatenate((np.ones(len(train_nls_c1)),np.zeros(len(train_nls_c3))), axis=None)
Avg_error_nls_c1_c3=np.array([])

n_epoch=1000
learning_factor=0.01
print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")
result=training_perceptron(n_epoch,train_nls_c1_c3,weights_nls_c1_c3,Target_label_nls_c1_c3,learning_factor)

Avg_error_nls_c1_c3=next(result)
weights_nls_c1_c3=next(result)

"""#####  Training Non-linear separable Class2-Class3 perceptron"""

train_nls_c2_c3=np.concatenate((train_nls_c2, train_nls_c3), axis=0)
Target_label_nls_c2_c3= np.concatenate((np.ones(len(train_nls_c2)),np.zeros(len(train_nls_c3))), axis=None)
Avg_error_nls_c2_c3=np.array([])

n_epoch=1000
learning_factor=0.01
print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")

result=training_perceptron(n_epoch,train_nls_c2_c3,weights_nls_c2_c3,Target_label_nls_c2_c3,learning_factor)

Avg_error_nls_c2_c3=next(result)
weights_nls_c2_c3=next(result)

"""##### Graph Plot Non linear separable data"""

fig, axs = plt.subplots(3,figsize=(7,5),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs[0].set_title('Class1-class2 Perceptron')
axs[0].plot(list(range(1,n_epoch+1)), Avg_error_nls_c1_c2)
axs[0].set(xlabel='No. of Epochs.', ylabel='Average Error')

axs[1].set_title('Class1-class3 Perceptron')
axs[1].plot(list(range(1,n_epoch+1)), Avg_error_nls_c1_c3)
axs[1].set(xlabel='No. of Epochs.', ylabel='Average Error')

axs[2].set_title('Class2-class3 Perceptron')
axs[2].plot(list(range(1,n_epoch+1)), Avg_error_nls_c2_c3)
axs[2].set(xlabel='No. of Epochs.', ylabel='Average Error')

"""##### Class1-class2 Decision Region Plot"""

X=np.delete(train_c1_c2,2,axis=1)
y=Target_label_c1_c2
weights=weights_c1_c2
markers = ('s', 'o')       #, '^', 'v')
colors = ('green', 'blue') #, 'lightgreen') #, 'gray', 'cyan')


plot_decision_regions(X, y, weights,markers,colors,test_idx=None, resolution=0.1)

"""##### Class1-class3 Decision Region Plot"""

X=np.delete(train_c1_c2,2,axis=1)
y=Target_label_c1_c2
weights=weights_c1_c2
markers = ('s', 'o')       #, '^', 'v')
colors = ('green', 'blue') #, 'lightgreen') #, 'gray', 'cyan')


plot_decision_regions(X, y, weights,markers,colors,test_idx=None, resolution=0.1)

"""##### Class2-class3 Decision Region Plot"""

X=np.delete(train_c1_c2,2,axis=1)
y=Target_label_c1_c2
weights=weights_c1_c2
markers = ('s', 'o')       #, '^', 'v')
colors = ('green', 'blue') #, 'lightgreen') #, 'gray', 'cyan')


plot_decision_regions(X, y, weights,markers,colors,test_idx=None, resolution=0.1)

"""# Regression Task

#### 1-D Univariate input Data
"""

Data_uni='/content/drive/MyDrive/Deep_learning/Group09_Assignment1/Regression/UnivariateData/9.csv'

univariate_data=np.loadtxt(Data_uni,delimiter=",", dtype=float)
univariate_data_points, univariate_actual_values= np.split(univariate_data, 2, axis=1)
univariate_data_points=np.insert(univariate_data_points.astype(float), 1, 1, axis=1)

univariate_train,univariate_test = np.split(univariate_data_points,[int(0.7 * len(univariate_data_points))])
Actual_train_value,Actual_test_value=  np.split(univariate_actual_values,[int(0.7 * len(univariate_actual_values))])

"""#### Regression on Univariate Data"""

weights_uni=np.array([0.4,0.4]) # [w1,w2,w0]
Avg_error_uni=np.array([])

n_epoch=100
learning_factor=0.05
print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")

counter=0
while counter!=n_epoch:
  Error_uni=np.array([])
  for i in np.random.permutation(len(univariate_train)):
    predicted_value=np.dot(univariate_train[i],weights_uni)
    error=0.5*(Actual_train_value[i]-predicted_value)**2
    Error_uni=np.append(Error_uni, error)
    delta_weight=(learning_factor*(Actual_train_value[i]-predicted_value)*predicted_value*(1-predicted_value))*univariate_train[i]
    weights_uni=np.add(weights_uni, delta_weight)
  Average_error=np.mean(Error_uni, axis = 0)  
  Avg_error_uni=np.append(Avg_error_uni, Average_error) 
  print("At Epoch="+str(counter+1)+", the average error is calculated as: "+str(Average_error))
  
  counter=counter+1

"""#### 2-D Bivariate input Data """

Data_Bi='/content/drive/MyDrive/Deep_learning/Group09_Assignment1/Regression/BivariateData/9.csv'

bivariate_data=np.loadtxt(Data_Bi,delimiter=",", dtype=float)
bivariate_data_points, bivariate_actual_values= np.split(bivariate_data, [2], axis=1)
bivariate_data_points=np.insert(bivariate_data_points.astype(float), 2, 1, axis=1)

bivariate_train,bivariate_test = np.split(bivariate_data_points,[int(0.7 * len(bivariate_data_points))])
Actual_train_value,Actual_test_value=  np.split(bivariate_actual_values,[int(0.7 * len(bivariate_actual_values))])

weights_bi=np.array([0.4,0.4,0.4]) # [w1,w2,w0] between the input range
Avg_error_bi=np.array([])

n_epoch=100
learning_factor=0.05
print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")

counter=0
while counter!=n_epoch:
  Error_bi=np.array([])
  for i in np.random.permutation(len(bivariate_train)):
    predicted_value=np.dot(bivariate_train[i],weights_bi)
    error=0.5*(Actual_train_value[i]-predicted_value)**2
    Error_bi=np.append(Error_bi, error)
    delta_weight=(learning_factor*(Actual_train_value[i]-predicted_value)*predicted_value*(1-predicted_value))*bivariate_train[i]
    weights_bi=np.add(weights_bi, delta_weight)
  Average_error=np.mean(Error_bi, axis = 0)  
  Avg_error_bi=np.append(Avg_error_bi, Average_error) 
  print("At Epoch="+str(counter+1)+", the average error is calculated as: "+str(Average_error))
  
  counter=counter+1

"""#### Graph Plot"""

fig, axs = plt.subplots(2,figsize=(7,5),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs[0].set_title('Univariate Regression Perceptron')
axs[0].plot(list(range(1,n_epoch+1)), Avg_error_uni)
axs[0].set(xlabel='No. of Epochs.', ylabel='Average Error')

axs[1].set_title('Bivariate Regression Perceptron')
axs[1].plot(list(range(1,n_epoch+1)), Avg_error_bi)
axs[1].set(xlabel='No. of Epochs.', ylabel='Average Error')

"""#### Mean Squared Error Univariate data"""

univariate_data=np.loadtxt(Data_uni,delimiter=",", dtype=float)
univariate_data_points, univariate_actual_values= np.split(univariate_data, 2, axis=1)
univariate_data_points=np.insert(univariate_data_points.astype(float), 1, 1, axis=1)

Error_uni_mse=np.array([])
Mean_Squared_Error=np.array([])
for i in np.random.permutation(len(univariate_data_points)):
  predicted_value=np.dot(univariate_data_points[i],weights_uni)
  error=(univariate_actual_values[i]-predicted_value)**2 ## Check
  Error_uni_mse=np.append(Error_uni_mse, error)
  Mean_Squared_Error=np.append( Mean_Squared_Error,np.mean(Error_uni_mse))

# generate random vectors
univariate_data_points, univariate_actual_values= np.split(univariate_data, 2, axis=1)
x = univariate_data_points
y = Mean_Squared_Error
print(univariate_data_points)
print( Mean_Squared_Error)
plt.scatter(x, y, alpha=0.5)
# compute slope m and intercept b
m,b = weights_uni
x=np.linspace(0,10,50)      # from 1 to 10, by 50
plt.plot(x, m*x + b)        # abline
plt.show()

train_c1_c2=np.concatenate((train_c1, train_c2), axis=0)
Target_label_c1_c2= np.concatenate((np.ones(len(train_c1)),np.zeros(len(train_c2))), axis=None)
Avg_error_c1_c2=np.array([])

print("<-----------------No. of Epoch=50----------------->")
n_epoch=1000
learning_factor=0.05

def training_perceptron(n_epoch,train_c1_c2,weights_c1_c2,Target_label_c1_c2):
  while n_epoch!=0:
    Error_c1_c2=np.array([])
    for i in np.random.permutation(len(train_c1_c2)):
      predicted_value=Activation_value(train_c1_c2[i],weights_c1_c2)
      error=0.5*(Target_label_c1_c2[i]-predicted_value)**2
      Error_c1_c2=np.append(Error_c1_c2, error)
      delta_weight=(learning_factor*(Target_label_c1_c2[i]-predicted_value)*predicted_value*(1-predicted_value))*train_c1_c2[i]
      weights_c1_c2=np.add(weights_c1_c2, delta_weight)
    Average_error=np.mean(Error_c1_c2, axis = 0)  
    Avg_error_c1_c2=np.append(Avg_error_c1_c2, Average_error) 
    print("At Epoch="+str(n_epoch)+", the average error is calculated as: "+str(Average_error))
    
    n_epoch = n_epoch-1

    return Avg_error_c1_c2