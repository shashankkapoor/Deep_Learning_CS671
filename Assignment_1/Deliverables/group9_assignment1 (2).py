# -*- coding: utf-8 -*-
"""Group9_Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c309RETyHOfX6SPK5PpXBeV7IQkClwkr

#### Mounting Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""#### Importing libraries"""

import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import numpy as np
import math

"""# Classification Task

#### linearly separable Data
"""

class_1 = '/content/drive/MyDrive/Deep_learning/Group09_Assignment1/Classification/LS_Group09/Class1.txt'
class_2 = '/content/drive/MyDrive/Deep_learning/Group09_Assignment1/Classification/LS_Group09/Class2.txt'
class_3 = '/content/drive/MyDrive/Deep_learning/Group09_Assignment1/Classification/LS_Group09/Class3.txt'

data_class_1 = np.loadtxt(class_1, delimiter=' ', skiprows=0, dtype=str)
data_class_2 = np.loadtxt(class_2, delimiter=' ', skiprows=0, dtype=str)
data_class_3 = np.loadtxt(class_3, delimiter=' ', skiprows=0, dtype=str)

data_class_1=np.insert(data_class_1.astype(float), 2, 1, axis=1)
data_class_2=np.insert(data_class_2.astype(float), 2, 1, axis=1)
data_class_3=np.insert(data_class_3.astype(float), 2, 1, axis=1)

train_c1,test_c1 = np.split(data_class_1,[int(0.7 * len(data_class_1))])
train_c2,test_c2 = np.split(data_class_2,[int(0.7 * len(data_class_2))])
train_c3,test_c3 = np.split(data_class_3,[int(0.7 * len(data_class_3))])

def training_perceptron(n_epoch,training_data,weights,target_training_label,learning_factor):
  Avg_error_list=np.array([])
  
  counter=0
  while counter!=n_epoch:
    Error_list=np.array([])
    for i in np.random.permutation(len(training_data)):
      predicted_value=Activation_value(training_data[i],weights)
      error=0.5*(target_training_label[i]-predicted_value)**2
      Error_list=np.append(Error_list, error)
      delta_weight=(learning_factor*(target_training_label[i]-predicted_value)*predicted_value*(1-predicted_value))*training_data[i]
      weights=np.add(weights, delta_weight)
    Average_error=np.mean(Error_list, axis = 0)  
    Avg_error_list=np.append(Avg_error_list, Average_error) 
    #print("At Epoch="+str(counter+1)+", the average error is calculated as: "+str(Average_error))
    
    counter=counter+1
  yield Avg_error_list
  yield weights

def plot_decision_regions(X, y,weights,markers,colors, test_idx=None, resolution=0.1):

    cmap = ListedColormap(colors[:len(np.unique(y))])

    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))
    
    
    Z= np.array([xx1.ravel(), xx2.ravel()]).T
    data_class=np.insert(Z.astype(float), 2, 1, axis=1)
    predict=np.array([])
    for i in range(len(Z)): 
      predict=np.append(predict,np.dot(data_class[i],weights))

    predict=predict.reshape(xx1.shape)
    plt.contourf(xx1, xx2, predict, alpha=0.3, cmap=cmap)

    plt.scatter(train_c1[:, 0], train_c1[:, 1], label='Class 1', edgecolors='white')
    plt.scatter(class2_train[:, 0], class2_train[:, 1], label='Class 2', edgecolors='white')
    # plt.scatter(class3_train[:, 0], class3_train[:, 1], label='Class 3', edgecolors='white')
    # plt.legend()
    # plt.title('Decision region learned by the classifier')

    # for idx, cl in enumerate(np.unique(y)):
    #     plt.scatter(x=X[y == cl, 0], 
    #                 y=X[y == cl, 1],
    #                 alpha=0.8, 
    #                 c=colors[idx],
    #                 marker=markers[idx], 
    #                 label=cl, 
    #                 edgecolor='white')

    # # highlight test samples
    # if test_idx:
    #     # plot all samples
    #     X_test, y_test = X[test_idx, :], y[test_idx]

    #     plt.scatter(X_test[:, 0],
    #                 X_test[:, 1],
    #                 c='',
    #                 edgecolor='black',
    #                 alpha=1.0,
    #                 linewidth=1,
    #                 marker='o',
    #                 s=100, 
    #                 label='test set')

"""##### linearly separable data"""

fig, ax = plt.subplots(1,2,figsize=(15,4), sharex=True)
ax[0].scatter(np.delete(train_c1,[1,2],axis=1) ,np.delete(train_c1,[0,2],axis=1) , color='red', label='class 1')
ax[0].scatter(np.delete(train_c2,[1,2],axis=1) ,np.delete(train_c2,[0,2],axis=1), color='blue', label='class 2')
ax[0].scatter(np.delete(train_c3,[1,2],axis=1) ,np.delete(train_c3,[0,2],axis=1), color='green', label='class 3')
ax[0].legend()
ax[0].set_xlabel('X-axis')
ax[0].set_ylabel('Y-axis')
# Show the plot
ax[0].title.set_text("linearly separable 70% Training data set") #ax1.title.set_text("First")
ax[1].scatter(np.delete(test_c1,[1,2],axis=1) ,np.delete(test_c1,[0,2],axis=1) , color='red', label='class 1')
ax[1].scatter(np.delete(test_c2,[1,2],axis=1) ,np.delete(test_c2,[0,2],axis=1), color='blue', label='class 2')
ax[1].scatter(np.delete(test_c3,[1,2],axis=1) ,np.delete(test_c3,[0,2],axis=1), color='green', label='class 3')
ax[1].legend()
ax[1].set_xlabel('X-axis')
ax[1].set_ylabel('Y-axis')
ax[1].title.set_text("linearly separable 30% Testing data set")
plt.show()

"""##### Training Class1-Class2 perceptron"""

train_c1_c2=np.concatenate((train_c1, train_c2), axis=0)
Target_label_c1_c2= np.concatenate((np.ones(len(train_c1)),np.zeros(len(train_c2))), axis=None)
weights_c1_c2=np.array([0.4,0.4,0.4]) 

n_epoch=1000
learning_factor=0.05

print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")
result=training_perceptron(n_epoch,train_c1_c2,weights_c1_c2,Target_label_c1_c2,learning_factor)

Avg_error_c1_c2=next(result)
weights_c1_c2=next(result)

"""##### Training Class1-Class3 perceptron"""

train_c1_c3=np.concatenate((train_c1, train_c3), axis=0)
Target_label_c1_c3= np.concatenate((np.ones(len(train_c1)),np.zeros(len(train_c3))), axis=None)
weights_c1_c3=np.array([0.4,0.4,0.4]) 

n_epoch=1000
learning_factor=0.05

print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")
result=training_perceptron(n_epoch,train_c1_c3,weights_c1_c3,Target_label_c1_c3,learning_factor)

Avg_error_c1_c3=next(result)
weights_c1_c3=next(result)

"""##### Training Class2-Class3 perceptron"""

train_c2_c3=np.concatenate((train_c2, train_c3), axis=0)
Target_label_c2_c3= np.concatenate((np.ones(len(train_c2)),np.zeros(len(train_c3))), axis=None)
weights_c2_c3=np.array([0.4,0.4,0.4]) 

n_epoch=1000
learning_factor=0.05

print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")
result=training_perceptron(n_epoch,train_c2_c3,weights_c2_c3,Target_label_c2_c3,learning_factor)

Avg_error_c2_c3=next(result)
weights_c2_c3=next(result)

"""##### Graph Plot linear Separable Data"""

fig, axs = plt.subplots(3,figsize=(10,5),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs[0].set_title('Class1-class2 Perceptron')
axs[0].plot(list(range(1,n_epoch+1)), Avg_error_c1_c2)
axs[0].set(xlabel='No. of Epochs.', ylabel='Average Error')

axs[1].set_title('Class1-class3 Perceptron')
axs[1].plot(list(range(1,n_epoch+1)), Avg_error_c1_c3)
axs[1].set(xlabel='No. of Epochs.', ylabel='Average Error')

axs[2].set_title('Class2-class3 Perceptron')
axs[2].plot(list(range(1,n_epoch+1)), Avg_error_c2_c3)
axs[2].set(xlabel='No. of Epochs.', ylabel='Average Error')

"""##### Class1-class2 Decision Region Plot"""

X=np.delete(train_c1_c2,2,axis=1)
y=Target_label_c1_c2
weights=weights_c1_c2

colors = ('green', 'blue') 
cmap = ListedColormap(colors[:len(np.unique(y))])
x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.2), np.arange(x2_min, x2_max, 0.2))

Z= np.array([xx1.ravel(), xx2.ravel()]).T
data_class=np.insert(Z.astype(float), 2, 1, axis=1)
predict=np.array([])

for i in range(len(Z)): 
  predict=np.append(predict,np.dot(data_class[i],weights))

predict=predict.reshape(xx1.shape)
plt.contourf(xx1, xx2, predict, alpha=0.3, cmap=cmap)
plt.scatter(train_c1[:, 0], train_c1[:, 1], color='magenta' ,label='Class 1', edgecolors='black')
plt.scatter(train_c2[:, 0], train_c2[:, 1], color='orange',label='Class 2', edgecolors='black')
plt.legend()
plt.title('Decision region between class1-class2')
plt.show()

"""##### Class1-class3 Decision Region Plot"""

X=np.delete(train_c1_c3,2,axis=1)
y=Target_label_c1_c3
weights=weights_c1_c3

colors = ('red', 'blue') 
cmap = ListedColormap(colors[:len(np.unique(y))])
x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.2), np.arange(x2_min, x2_max, 0.2))

Z= np.array([xx1.ravel(), xx2.ravel()]).T
data_class=np.insert(Z.astype(float), 2, 1, axis=1)
predict=np.array([])
for i in range(len(Z)): 
  predict=np.append(predict,np.dot(data_class[i],weights))

predict=predict.reshape(xx1.shape)
plt.contourf(xx1, xx2, predict, alpha=0.3, cmap=cmap)
plt.scatter(train_c1[:, 0], train_c1[:, 1], color='magenta' ,label='Class 1', edgecolors='black')
plt.scatter(train_c3[:, 0], train_c3[:, 1], color='green',label='Class 3', edgecolors='black')
plt.legend()
plt.title('Decision region between class1-class3')
plt.show()

"""##### Class2-class3 Decision Region Plot"""

X=np.delete(train_c2_c3,2,axis=1)
y=Target_label_c2_c3
weights=weights_c2_c3

colors = ( 'red','green',) 
cmap = ListedColormap(colors[:len(np.unique(y))])
x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.2), np.arange(x2_min, x2_max, 0.2))

Z= np.array([xx1.ravel(), xx2.ravel()]).T
data_class=np.insert(Z.astype(float), 2, 1, axis=1)
predict=np.array([])
for i in range(len(Z)): 
  predict=np.append(predict,np.dot(data_class[i],weights))

predict=predict.reshape(xx1.shape)
plt.contourf(xx1, xx2, predict, alpha=0.3, cmap=cmap)

plt.scatter(train_c2[:, 0], train_c2[:, 1], color='orange' ,label='Class 2', edgecolors='black')
plt.scatter(train_c3[:, 0], train_c3[:, 1], color='green',label='Class 3', edgecolors='black')
plt.legend()
plt.title('Decision region between class2-class3')
plt.show()

"""##### Class1-class2-class3 Decision Region Plot"""

markers = ('s', 'x', 'o')#, '^', 'v')
colors = ('red', 'blue', 'lightgreen')#, 'gray', 'cyan')
y=[0,1,2]
cmap = ListedColormap(colors[:len(np.unique(y))])

X=np.delete(np.vstack([train_c1_c2,train_c2_c3]),2,axis=1)
weights_1=weights_c1_c2
weights_2=weights_c1_c3

x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.2),
                        np.arange(x2_min, x2_max, 0.2))

Z= np.array([xx1.ravel(), xx2.ravel()]).T
data_class=np.insert(Z.astype(float), 2, 1, axis=1)
predict=np.array([])

for i in range(len(Z)): 
  predict=np.append(predict,np.dot(data_class[i],weights_1))
for i in range(len(Z)): 
  predict1=np.append(predict1,np.dot(data_class[i],weights_2))

predict=predict.reshape(xx1.shape)
predict1=predict1.reshape(xx1.shape)

ax.contourf(xx1, xx2, predict, alpha=0.3, cmap=cmap)
ax.contourf(xx1, xx2, predict1, alpha=0.3, cmap=cmap)

plt.scatter(train_c1[:, 0], train_c1[:, 1], color='blue' ,label='Class 3', edgecolors='black')
plt.scatter(train_c2[:, 0], train_c2[:, 1], color='orange' ,label='Class 1', edgecolors='black')
plt.scatter(train_c3[:, 0], train_c3[:, 1], color='green',label='Class 2', edgecolors='black')

plt.legend()
plt.title('Decision region between class1-class2-class3')
plt.show()

X=np.delete(np.vstack([train_c1,train_c2,train_c3]),2,axis=1)
y = np.concatenate((np.zeros(len(train_c1)),np.ones(len(train_c2)), np.ones(len(train_c3)) * 2))
weights_1=weights_c1_c2
weights_2=weights_c1_c3

colors = ( 'red','green','blue') 
cmap = ListedColormap(colors[:len(np.unique(y))])

x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.2), np.arange(x2_min, x2_max, 0.2))

Z=np.array([xx1.ravel(), xx2.ravel()]).T
data_class=np.insert(Z.astype(float), 2, 1, axis=1)

predict=np.array([])
predict1=np.array([])

for i in range(len(Z)): 
  predict=np.append(predict,np.dot(data_class[i],weights_1))
for i in range(len(Z)):
   predict1=np.append(predict1,np.dot(data_class[i],weights_2))

predict=predict.reshape(xx1.shape)
predict1=predict1.reshape(xx1.shape)

# fig, ax = plt.subplots()
# ax.contourf(xx1, xx2, predict, alpha=0.3, cmap=cmap)
# ax.contourf(xx1, xx2, predict1, alpha=0.3, cmap=cmap)
fig, ax = plt.subplots()
ax.contourf(xx1, xx2, predict, cmap=plt.cm.Accent, alpha=0.4)
ax.contourf(xx1, xx2, predict1, cmap=plt.cm.Paired, alpha=0.4)

ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolors='k')
plt.show()

# ax.scatter(train_c1[:, 0], train_c1[:, 1], color='blue' ,label='Class 3', edgecolors='black')
# ax.scatter(train_c2[:, 0], train_c2[:, 1], color='orange' ,label='Class 1', edgecolors='black')
# ax.scatter(train_c3[:, 0], train_c3[:, 1], color='green',label='Class 2', edgecolors='black')

# plt.legend()
# plt.title('Decision region between class1-class2-class3')
# plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Generate random data for three classes
# n = 500
# X1 = np.random.normal(loc=[-2, -2], scale=[1, 1], size=(n, 2))
# X2 = np.random.normal(loc=[2, -2], scale=[1, 1], size=(n, 2))
# X3 = np.random.normal(loc=[0, 2], scale=[1, 1], size=(n, 2))

# # Create labels for each class
# y1 = np.zeros(n)
# y2 = np.ones(n)
# y3 = np.ones(n) * 2

# Concatenate data and labels
X=np.delete(np.vstack([train_c1,train_c2,train_c3]),2,axis=1)
y =np.concatenate((np.zeros(len(train_c1)),np.ones(len(train_c2)), np.ones(len(train_c3)) * 2))

# Define mesh grid range
h = 0.02  # step size in the mesh
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Define classifiers for each class
# classifier1 = np.random.rand(2)  # Random weights for linear classifier 1
# classifier2 = np.random.rand(2)  # Random weights for linear classifier 2
# classifier3 = np.random.rand(2)  # Random weights for linear classifier 3

# Predict the mesh grid values for each class
Z1 = np.dot(np.c_[xx.ravel(), yy.ravel(),1], weights_c1_c2)
Z1 = Z1.reshape(xx.shape)
Z2 = np.dot(np.c_[xx.ravel(), yy.ravel(),1], weights_c1_c3)
Z2 = Z2.reshape(xx.shape)
Z3 = np.dot(np.c_[xx.ravel(), yy.ravel(),1], weights_c2_c3)
Z3 = Z3.reshape(xx.shape)

# Plot the decision mesh
fig, ax = plt.subplots()
ax.contourf(xx, yy, Z1, cmap=plt.cm.Accent, alpha=0.4)
ax.contourf(xx, yy, Z2, cmap=plt.cm.Paired, alpha=0.4)
ax.contourf(xx, yy, Z3, cmap=plt.cm.Set1, alpha=0.4)
ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolors='k')
plt.show()

def Activation_value(x,w):
  z=np.dot(x,w)
  return Output_signal(z)

def Output_signal(z):
   return (1/(1+np.exp(-z)))

"""##### Confusion Matrix"""

Test_Data = np.concatenate((test_c1, test_c2, test_c3), axis=0)
True_labels = np.concatenate((np.full(shape=test_c1.shape[0], fill_value=1),np.full(shape=test_c2.shape[0], fill_value=2), np.full(shape=test_c3.shape[0], fill_value=3)), axis=0)   
Test_Data.shape, True_labels.shape

predicted_labels = []
for sample in Test_Data:
    predicted_labels.append(l_classifier.predict(sample=sample))

from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, recall_score
print(f'Confusion matrix:\n{confusion_matrix(all_true_labels, all_pred_labels)}')
print(f'Acccuracy: {accuracy_score(all_true_labels, all_pred_labels)}')
print(f'Recall: {recall_score(all_true_labels, all_pred_labels, labels=(1, 2, 3), average="micro")}')
print(f'f1-score: {f1_score(all_true_labels, all_pred_labels, labels=(1, 2, 3), average="micro")}')

"""#### Non-linearly separable data"""

path_nls ='/content/drive/MyDrive/Deep_learning/Group09_1/Classification/NLS_Group09.txt'

Data_nls= np.loadtxt(path_nls, delimiter=' ', skiprows=1, dtype=str)

data_class1_nls,data_class2_nls, data_class3_nls=np.split(Data_nls,[500,1000])

data_class1_nls=np.delete(data_class1_nls,2,axis=1)
data_class2_nls=np.delete(data_class2_nls,2,axis=1)
data_class3_nls=np.delete(data_class3_nls,2,axis=1)

data_class1_nls=np.insert(data_class1_nls.astype(float), 2, 1, axis=1)
data_class2_nls=np.insert(data_class2_nls.astype(float), 2, 1, axis=1)
data_class3_nls=np.insert(data_class3_nls.astype(float), 2, 1, axis=1)

train_nls_c1,test_nls_c1 = np.split(data_class1_nls,[int(0.7 * len(data_class1_nls))])
train_nls_c2,test_nls_c2 = np.split(data_class2_nls,[int(0.7 * len(data_class2_nls))])
train_nls_c3,test_nls_c3 = np.split(data_class3_nls,[int(0.7 * len(data_class3_nls))])

weights_nls_c1_c2=np.array([0.4,0.4,0.4]) # [w1,w2,w0] between the input range
weights_nls_c1_c3=np.array([0.4,0.4,0.4]) # [w1,w2,w0]
weights_nls_c2_c3=np.array([0.4,0.4,0.4]) # [w1,w2,w0]

Error_nls_c1_c2=np.array([])
Error_nls_c1_c3=np.array([])
Error_nls_c2_c3=np.array([])

Avg_error_nls_c1_c2=np.array([])
Avg_error_nls_c1_c3=np.array([])
Avg_error_nls_c2_c3=np.array([])

"""##### Non linearly separable data"""

fig, ax = plt.subplots(1,2,figsize=(15,4), sharex=True)
ax[0].scatter(np.delete(train_nls_c1,[1,2],axis=1) ,np.delete(train_nls_c1,[0,2],axis=1) , color='red', label='class 1')
ax[0].scatter(np.delete(train_nls_c2,[1,2],axis=1) ,np.delete(train_nls_c2,[0,2],axis=1), color='blue', label='class 2')
ax[0].scatter(np.delete(train_nls_c3,[1,2],axis=1) ,np.delete(train_nls_c3,[0,2],axis=1), color='green', label='class 3')
ax[0].legend()
ax[0].set_xlabel('X-axis')
ax[0].set_ylabel('Y-axis')
# Show the plot
ax[0].title.set_text("linearly separable 70% Training data set") #ax1.title.set_text("First")
ax[1].scatter(np.delete(test_nls_c1,[1,2],axis=1) ,np.delete(test_nls_c1,[0,2],axis=1) , color='red', label='class 1')
ax[1].scatter(np.delete(test_nls_c2,[1,2],axis=1) ,np.delete(test_nls_c2,[0,2],axis=1), color='blue', label='class 2')
ax[1].scatter(np.delete(test_nls_c3,[1,2],axis=1) ,np.delete(test_nls_c3,[0,2],axis=1), color='green', label='class 3')
ax[1].legend()
ax[1].set_xlabel('X-axis')
ax[1].set_ylabel('Y-axis')
ax[1].title.set_text("linearly separable 30% Testing data set")
plt.show()

"""##### Training Non-linear separable Class1-Class2 perceptron """

train_nls_c1_c2=np.concatenate((train_nls_c1, train_nls_c2), axis=0)
Target_label_nls_c1_c2= np.concatenate((np.ones(len(train_nls_c1)),np.zeros(len(train_nls_c2))), axis=None)
Avg_error_nls_c1_c2=np.array([])

n_epoch=1000
learning_factor=0.01

print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")
result=training_perceptron(n_epoch,train_nls_c1_c2,weights_nls_c1_c2,Target_label_nls_c1_c2,learning_factor)

Avg_error_nls_c1_c2=next(result)
weights_nls_c1_c2=next(result)

"""##### Training Non-linear separable Class1-Class3 perceptron"""

train_nls_c1_c3=np.concatenate((train_nls_c1, train_nls_c3), axis=0)
Target_label_nls_c1_c3= np.concatenate((np.ones(len(train_nls_c1)),np.zeros(len(train_nls_c3))), axis=None)
Avg_error_nls_c1_c3=np.array([])

n_epoch=1000
learning_factor=0.01
print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")
result=training_perceptron(n_epoch,train_nls_c1_c3,weights_nls_c1_c3,Target_label_nls_c1_c3,learning_factor)

Avg_error_nls_c1_c3=next(result)
weights_nls_c1_c3=next(result)

"""#####  Training Non-linear separable Class2-Class3 perceptron"""

train_nls_c2_c3=np.concatenate((train_nls_c2, train_nls_c3), axis=0)
Target_label_nls_c2_c3= np.concatenate((np.ones(len(train_nls_c2)),np.zeros(len(train_nls_c3))), axis=None)
Avg_error_nls_c2_c3=np.array([])

n_epoch=1000
learning_factor=0.01
print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")

result=training_perceptron(n_epoch,train_nls_c2_c3,weights_nls_c2_c3,Target_label_nls_c2_c3,learning_factor)

Avg_error_nls_c2_c3=next(result)
weights_nls_c2_c3=next(result)

"""##### Graph Plot Non linear separable data"""

fig, axs = plt.subplots(3,figsize=(7,5),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.0,right=0.9, top=1.5,wspace=0.8,hspace=0.4)

axs[0].set_title('Class1-class2 Perceptron')
axs[0].plot(list(range(1,n_epoch+1)), Avg_error_nls_c1_c2)
axs[0].set(xlabel='No. of Epochs.', ylabel='Average Error')

axs[1].set_title('Class1-class3 Perceptron')
axs[1].plot(list(range(1,n_epoch+1)), Avg_error_nls_c1_c3)
axs[1].set(xlabel='No. of Epochs.', ylabel='Average Error')

axs[2].set_title('Class2-class3 Perceptron')
axs[2].plot(list(range(1,n_epoch+1)), Avg_error_nls_c2_c3)
axs[2].set(xlabel='No. of Epochs.', ylabel='Average Error')

"""##### Class1-class2 Decision Region Plot"""

X=np.delete(train_nls_c1_c2,2,axis=1)
y=Target_label_nls_c1_c2
weights=weights_nls_c1_c2
markers = ('s', 'o')       #, '^', 'v')
colors = ('green', 'blue') #, 'lightgreen') #, 'gray', 'cyan')


plot_decision_regions(X, y, weights,markers,colors,test_idx=None, resolution=0.1)

"""##### Class1-class3 Decision Region Plot"""

X=np.delete(train_nls_c1_c3,2,axis=1)
y=Target_label_nls_c1_c3
weights=weights_nls_c1_c3
markers = ('s', '^')       #, '^', 'v')
colors = ('green', 'Red') #, 'lightgreen') #, 'gray', 'cyan')


plot_decision_regions(X, y, weights,markers,colors,test_idx=None, resolution=0.1)

"""##### Class2-class3 Decision Region Plot"""

X=np.delete(train_nls_c2_c3,2,axis=1)
y=Target_label_nls_c2_c3
weights=weights_nls_c2_c3
markers = ( 'o','^')       #, '^', 'v')
colors = ( 'blue','red') #, 'lightgreen') #, 'gray', 'cyan')


plot_decision_regions(X, y, weights,markers,colors,test_idx=None, resolution=0.1)

"""##### class1-Class2-class3 Decision Region Plot"""



"""# Regression Task

#### 1-D Univariate input Data
"""

Data_uni='/content/drive/MyDrive/Deep_learning/Group09_Assignment1/Regression/UnivariateData/9.csv'

univariate_data=np.loadtxt(Data_uni,delimiter=",", dtype=float)
univariate_data_points, univariate_actual_values= np.split(univariate_data, 2, axis=1)
univariate_data_points=np.insert(univariate_data_points.astype(float), 1, 1, axis=1)

univariate_train,univariate_test = np.split(univariate_data_points,[int(0.7 * len(univariate_data_points))])
Actual_train_value,Actual_test_value=  np.split(univariate_actual_values,[int(0.7 * len(univariate_actual_values))])

"""#### Regression on Univariate Data"""

weights_uni=np.array([0.4,0.4]) # [w1,w2,w0]
Avg_error_uni=np.array([])

n_epoch=100
learning_factor=0.05
print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")

counter=0
while counter!=n_epoch:
  Error_uni=np.array([])
  for i in np.random.permutation(len(univariate_train)):
    predicted_value=np.dot(univariate_train[i],weights_uni)
    error=0.5*(Actual_train_value[i]-predicted_value)**2
    Error_uni=np.append(Error_uni, error)
    delta_weight=(learning_factor*(Actual_train_value[i]-predicted_value)*predicted_value*(1-predicted_value))*univariate_train[i]
    weights_uni=np.add(weights_uni, delta_weight)
  Average_error=np.mean(Error_uni, axis = 0)  
  Avg_error_uni=np.append(Avg_error_uni, Average_error) 
  #print("At Epoch="+str(counter+1)+", the average error is calculated as: "+str(Average_error))
  
  counter=counter+1

"""#### 2-D Bivariate input Data """

Data_Bi='/content/drive/MyDrive/Deep_learning/Group09_Assignment1/Regression/BivariateData/9.csv'

bivariate_data=np.loadtxt(Data_Bi,delimiter=",", dtype=float)
bivariate_data_points, bivariate_actual_values= np.split(bivariate_data, [2], axis=1)
bivariate_data_points=np.insert(bivariate_data_points.astype(float), 2, 1, axis=1)

bivariate_train,bivariate_test = np.split(bivariate_data_points,[int(0.7 * len(bivariate_data_points))])
Bi_Actual_train_value,Bi_Actual_test_value=  np.split(bivariate_actual_values,[int(0.7 * len(bivariate_actual_values))])

"""#### Regression on Bivariate Data"""

weights_bi=np.array([0.4,0.4,0.4]) # [w1,w2,w0] between the input range
Avg_error_bi=np.array([])

n_epoch=100
learning_factor=0.05
print("<-----------------No. of Epoch= "+str(n_epoch)+"----------------->")

counter=0
while counter!=n_epoch:
  Error_bi=np.array([])
  for i in np.random.permutation(len(bivariate_train)):
    predicted_value=np.dot(bivariate_train[i],weights_bi)
    error=0.5*(Bi_Actual_train_value[i]-predicted_value)**2
    Error_bi=np.append(Error_bi, error)
    delta_weight=(learning_factor*(Bi_Actual_train_value[i]-predicted_value)*predicted_value*(1-predicted_value))*bivariate_train[i]
    weights_bi=np.add(weights_bi, delta_weight)
  Average_error=np.mean(Error_bi, axis = 0)  
  Avg_error_bi=np.append(Avg_error_bi, Average_error) 
  print("At Epoch="+str(counter+1)+", the average error is calculated as: "+str(Average_error))
  
  counter=counter+1

"""#### Graph Plot"""

fig, axs = plt.subplots(2,figsize=(10,3),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs[0].set_title('Univariate Regression Perceptron')
axs[0].plot(list(range(1,n_epoch+1)), Avg_error_uni)
axs[0].set(xlabel='No. of Epochs.', ylabel='Average Error')

axs[1].set_title('Bivariate Regression Perceptron')
axs[1].plot(list(range(1,n_epoch+1)), Avg_error_bi)
axs[1].set(xlabel='No. of Epochs.', ylabel='Average Error')

"""#### Mean Squared Error Univariate data"""

Error_uni_mse=np.array([])
Mean_Squared_Error_train=0
for i in np.random.permutation(len(univariate_train)):
  predicted_value=np.dot(univariate_train[i],weights_uni)
  error=(Actual_train_value[i]-predicted_value)**2 ## Check
  Error_uni_mse=np.append(Error_uni_mse, error)
Mean_Squared_Error_train=np.mean(Error_uni_mse)

Error_uni_mse=np.array([])
Mean_Squared_Error_test=0
for i in np.random.permutation(len(univariate_test)):
  predicted_value=np.dot(univariate_test[i],weights_uni)
  error=(Actual_test_value[i]-predicted_value)**2 ## Check
  Error_uni_mse=np.append(Error_uni_mse, error)
Mean_Squared_Error_test=np.mean(Error_uni_mse)

"""##### Plot mean squared error """

xs = [1,2]
ys = [Mean_Squared_Error_train,Mean_Squared_Error_test]

labels = ['Training','Testing']

fig = plt.figure(figsize = (3, 5))
plt.bar(xs,ys, width=0.4,align='center',color='orange')
plt.xlabel("Phase")
plt.ylabel("Mean Squared Error")
plt.title("MSE for univariate data")
plt.xticks(xs,labels)

"""#### Mean Squared Error Bivariate data"""

Error_bi_mse=np.array([])
Mean_Squared_Error_train=0
for i in np.random.permutation(len(bivariate_train)):
  predicted_value=np.dot(bivariate_train[i],weights_bi)
  error=(Bi_Actual_train_value[i]-predicted_value)**2 ## Check
  Error_bi_mse=np.append(Error_bi_mse, error)
Mean_Squared_Error_train=np.mean(Error_bi_mse)

Error_bi_mse=np.array([])
Mean_Squared_Error_test=0
for i in np.random.permutation(len(bivariate_test)):
  predicted_value=np.dot(bivariate_test[i],weights_bi)
  error=(Bi_Actual_test_value[i]-predicted_value)**2 ## Check
  Error_bi_mse=np.append(Error_bi_mse, error)
Mean_Squared_Error_test=np.mean(Error_bi_mse)

"""#### Plot mean squared error"""

xs = [1,2]
ys = [Mean_Squared_Error_train,Mean_Squared_Error_test]

labels = ['Training','Testing']

fig = plt.figure(figsize = (3, 5))
plt.bar(xs,ys, width=0.4,align='center',color='orange')
plt.xlabel("Phase")
plt.ylabel("Mean Squared Error")
plt.title("MSE for bivariate data")
plt.xticks(xs,labels)

"""#### Plot for univariate model output and target output for training """

model_output=np.array([])
for i in range(len(univariate_train)):
   predicted_value=np.dot(univariate_train[i],weights_uni)
   model_output=np.append( model_output,predicted_value)

fig = plt.figure(figsize = (13, 5))
plt.scatter(np.delete(univariate_train,1,axis=1), Actual_train_value, color='lightblue' ,label='Actual output', edgecolors='green')
plt.scatter(np.delete(univariate_train,1,axis=1),model_output, color='orange' ,label='Targeted output', edgecolors='yellow')
plt.legend()
plt.xlabel('Feature value')
plt.ylabel('output value')
plt.title('Targeted and Actual output of univariate model')
plt.show()

"""#### Plot for univariate model output and target output for testing"""

model_output=np.array([])
for i in range(len(univariate_test)):
   predicted_value=np.dot(univariate_test[i],weights_uni)
   model_output=np.append( model_output,predicted_value)

fig = plt.figure(figsize = (13, 5))
plt.scatter(np.delete(univariate_test,1,axis=1), Actual_test_value, color='lightblue' ,label='Actual output', edgecolors='green')
plt.scatter(np.delete(univariate_test,1,axis=1),model_output, color='orange' ,label='Targeted output', edgecolors='yellow')
plt.legend()
plt.xlabel('Feature value')
plt.ylabel('output value')
plt.title('Targeted and Actual output of univariate model')
plt.show()

"""#### Plot for bivariate model output and target output for training"""

from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt


# Creating dataset
z = Bi_Actual_train_value
x = np.delete(bivariate_train,[0,2],axis=1) # 2nd feature
y = np.delete(bivariate_train,[1,2],axis=1) # 1st feature

model_output=np.array([])
for i in range(len(bivariate_train)):
   predicted_value=np.dot(bivariate_train[i],weights_bi)
   model_output=np.append(model_output,predicted_value)
 
fig = plt.figure(figsize = (12,8))
ax = plt.axes(projection ="3d")

ax.scatter3D(x, y, z, color = "green",edgecolors='lightgreen')
ax.scatter3D(x, y, model_output, color = "red",edgecolors='orange')
ax.set_xlabel("Second feature value ")
ax.set_ylabel("First feature value")
ax.set_zlabel(" Output value")
plt.title("Target and Actual output for bivariate")
 
# show plot
plt.show()

"""#### Plot for bivariate model output and target output for testing"""

from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt


# Creating dataset
z = Bi_Actual_test_value
x = np.delete(bivariate_test,[0,2],axis=1) # 2nd feature
y = np.delete(bivariate_test,[1,2],axis=1) # 1st feature

model_output=np.array([])
for i in range(len(bivariate_test)):
   predicted_value=np.dot(bivariate_test[i],weights_bi)
   model_output=np.append(model_output,predicted_value)
 
fig = plt.figure(figsize = (12,8))
ax = plt.axes(projection ="3d")

ax.scatter3D(x, y, z, color = "green",edgecolors='lightgreen')
ax.scatter3D(x, y, model_output, color = "red",edgecolors='orange')
ax.set_xlabel("Second feature value ")
ax.set_ylabel("First feature value")
ax.set_zlabel(" Output value")
plt.title("Target and Actual output for bivariate")
 
# show plot
plt.show()

"""#### univariate scatter plot model and target output for training data """

model_output=np.array([])
for i in range(len(univariate_train)):
   predicted_value=np.dot(univariate_train[i],weights_uni)
   model_output=np.append( model_output,predicted_value)

fig = plt.figure(figsize = (13, 5))
plt.scatter(Actual_train_value,model_output, color='lightblue' , edgecolors='green')
plt.xlabel('Actual Train value')
plt.ylabel('Model Output')
plt.title('Targeted and Actual output of univariate model')
plt.show()

"""#### univariate scatter plot model and target output for testing data"""

model_output=np.array([])
for i in range(len(univariate_test)):
   predicted_value=np.dot(univariate_test[i],weights_uni)
   model_output=np.append( model_output,predicted_value)

fig = plt.figure(figsize = (13, 5))
plt.scatter(Actual_test_value,model_output,color = "red",edgecolors='orange')
plt.xlabel('Actual Train value')
plt.ylabel('Model Output')
plt.title('Targeted and Actual output of univariate model')
plt.show()

"""#### Bivariate scatter plot model and target output for training data"""

model_output=np.array([])
for i in range(len(bivariate_train)):
   predicted_value=np.dot(bivariate_train[i],weights_bi)
   model_output=np.append( model_output,predicted_value)

fig = plt.figure(figsize = (13, 5))
plt.scatter(Bi_Actual_train_value,model_output,color='lightblue',edgecolors='green')
plt.xlabel('Actual Train value')
plt.ylabel('Model Output')
plt.title('Targeted and Actual output of Bivariate model')
plt.show()

"""#### Bivariate scatter plot model and target output for testing data"""

model_output=np.array([])
for i in range(len(bivariate_test)):
   predicted_value=np.dot(bivariate_test[i],weights_bi)
   model_output=np.append( model_output,predicted_value)

fig = plt.figure(figsize = (13, 5))
plt.scatter(Bi_Actual_test_value,model_output,color = "red",edgecolors='orange')
plt.xlabel('Actual Train value')
plt.ylabel('Model Output')
plt.title('Targeted and Actual output of Bivariate model')
plt.show()