# -*- coding: utf-8 -*-
"""Group09_Assignment6_code_Handwritten.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11763BXkP_JT9V0QSGWQi0qK2GMDqFs1E
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pickle
import cv2
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from keras.models import Model
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Dropout

from keras.layers import Masking, SimpleRNN, Dense,LSTM

"""#### Pre processing

"""

def normalizing(data):

  max_value=np.max(data[:,0])
  min_value=np.min(data[:,0])
  diff=max_value-min_value
  data[:,0]=(data[:,0]-min_value)/diff
  
  max_value=np.max(data[:,1])
  min_value=np.min(data[:,1])
  diff=max_value-min_value
  data[:,1]=(data[:,1]-min_value)/diff

  return data

def preprocessing_data(subdir):
  Dataset_alphabets= ["a","bA","dA","lA","tA"]
  path="/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/Handwriting_Data"
  length=[]
  data=[]
  label=[]
  for i in range(len(Dataset_alphabets)):
    for filenames in os.listdir(path+"/"+Dataset_alphabets[i]+"/"+subdir):
            file=os.path.join(path+"/"+Dataset_alphabets[i]+"/"+subdir, filenames)
            with open(file, 'r') as file:
                  contents = file.read()
                  values = contents.split()
                  numerical_values = [float(value) for value in values]
            length.append(numerical_values[0])
            values=numerical_values[1:]
            values=(np.array(values).reshape((-1,2)))
            values=normalizing(values)
            #print(type(values),values)
            data.append(values)
            label.append(i)
    print(Dataset_alphabets[i] +":"+ str(i))

  return np.array(data),np.array(label),np.array(length)

"""#### storing the dataset"""

x_train,y_train,length_train=preprocessing_data(subdir="train")
x_test,y_test,length_test=preprocessing_data(subdir="dev")

print(np.max(length_train))
print(np.max(length_test))

mask_value=2.0
x_train_pad = tf.keras.utils.pad_sequences(x_train, dtype=np.float64, padding="post", value=mask_value, maxlen=178)
x_test_pad = tf.keras.utils.pad_sequences(x_test, dtype=np.float64, padding="post", value=mask_value, maxlen=178)

with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/Dataset/x_train.pkl", 'wb') as f:
    pickle.dump(x_train_pad, f)
with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/Dataset/y_train.pkl", 'wb') as f:
    pickle.dump(y_train, f)

with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/Dataset/x_test.pkl", 'wb') as f:
    pickle.dump(x_test_pad, f)
with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/Dataset/y_test.pkl", 'wb') as f:
    pickle.dump(y_test, f)

"""#### Loading the Dataset """

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/Dataset/x_train.pkl', 'rb') as f:
    x_train = pickle.load(f)
with open('/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/Dataset/y_train.pkl', 'rb') as f:
    y_train = pickle.load(f)

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/Dataset/x_test.pkl', 'rb') as f:
    x_test = pickle.load(f)
with open('/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/Dataset/y_test.pkl', 'rb') as f:
    y_test = pickle.load(f)

np.array(x_train).shape

"""#### Neural Architecture

##### Simple RNN
"""

early_stopping = EarlyStopping(monitor='loss', patience=50, min_delta=0.0001)

mask_value=2.0

model = Sequential()
model.add(Masking(mask_value=mask_value, input_shape=(178, 2)))
model.add(SimpleRNN(32,activation='tanh',return_sequences=True))
model.add(SimpleRNN(64,activation='tanh',return_sequences=True))
model.add(SimpleRNN(128,activation='tanh',return_sequences=True))
model.add(SimpleRNN(256,activation='tanh',return_sequences=False))
model.add(Dense(128, activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(64, activation="Softmax"))

model.summary()

Adam_optimizer=Adam(learning_rate=0.0001, beta_1=0.9,beta_2=0.999, epsilon=1e-08, name="Adam")

model.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

history = model.fit(x_train, y_train, shuffle=True,callbacks=[early_stopping],epochs=500000,validation_split=0.0)

with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/results/RNN_Architecure_32_64_128_256_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

model.save(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/results/RNN_Architecure_32_64_128_256_model.h5")

"""#### Results"""

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/results/RNN_Architecure_32_64_history.pkl', 'rb') as f:
    history = pickle.load(f)

print("Total No. of Epochs to converge "+ str(len(history['accuracy']))+"\n")
print("Training Accuracy "+ str(history['accuracy'][-1]))

fig, axs = plt.subplots(1,2,figsize=(14,2),sharex=True, sharey=False)

axs[0].set_title('RNN_Architecture')
axs[0].plot(list(range(1,len(history['loss'])+1 )), history['loss'])
axs[0].set(xlabel='No. of Epochs.',ylabel= 'loss')
axs[1].set_title('RNN_Architecture')
axs[1].plot(list(range(1,len(history['accuracy'])+1 )), history['accuracy'])
axs[1].set(xlabel='No. of Epochs.',ylabel= 'Accuracy')

plt.show()

model = keras.models.load_model('/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/results/RNN_Architecure_32_64_model.h5')

predictions = model.predict(x_test)
conf_matrix=tf.math.confusion_matrix(y_test,predictions.argmax(axis=1))

accuracy = tf.metrics.Accuracy()(y_test, predictions.argmax(axis=1))

print("Confusion matrix:\n", conf_matrix.numpy())
print("Accuracy:", accuracy.numpy())

"""##### Handwritten plots"""

x_train,y_train,_=preprocessing_data(subdir="train")

import random
Dataset_alphabets= ["a","bA","dA","lA","tA"]

characters = [
    [random.randint(0, 69) for _ in range(5)],   # Character 1
    [random.randint(75, 138) for _ in range(5)], # Character 2
    [random.randint(145, 200) for _ in range(5)],# Character 3
    [random.randint(220, 260) for _ in range(5)],# Character 4
    [random.randint(290, 340) for _ in range(5)] # Character 5
]

fig, ax = plt.subplots(5, 5, figsize=(15, 15),sharex=False, sharey=True,gridspec_kw={'hspace': 0.5})
for i, char in enumerate(characters):
    for j, idx in enumerate(char):
        data = x_train[idx]
        ax[i, j].plot(data[:, 0], data[:, 1])
        ax[i, j].set_title(Dataset_alphabets[y_train[idx]])
plt.show()

"""##### LSTM"""

early_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=0.0001)

mask_value=2.0

model = Sequential()
model.add(Masking(mask_value=mask_value, input_shape=(178, 2)))
model.add(LSTM(32,activation='tanh',return_sequences=True))
model.add(LSTM(64,activation='tanh',return_sequences=True))
model.add(LSTM(128,activation='tanh',return_sequences=True))
model.add(LSTM(256,activation='tanh',return_sequences=False))
model.add(Dense(128, activation="relu"))
model.add(Dropout(0.2))
model.add(Dense(64, activation="Softmax"))

model.summary()

Adam_optimizer=Adam(learning_rate=0.0001, beta_1=0.9,beta_2=0.999, epsilon=1e-08, name="Adam")

model.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

history=model.fit(x_train, y_train, shuffle=True,callbacks=[early_stopping],epochs=500000,validation_split=0.0)

with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/results/LSTM_Architecure_32_64_128_256_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

model.save(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/results/LSTM_Architecure_32_64_128_256_model.h5")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/results/LSTM_Architecure_32_64_128_256_history.pkl', 'rb') as f:
    history = pickle.load(f)

print("Total No. of Epochs to converge "+ str(len(history['accuracy']))+"\n")
print("Training Accuracy "+ str(history['accuracy'][-1]))

fig, axs = plt.subplots(1,2,figsize=(14,2),sharex=True, sharey=False)

axs[0].set_title('LSTM_Architecture')
axs[0].plot(list(range(1,len(history['loss'])+1 )), history['loss'])
axs[0].set(xlabel='No. of Epochs.',ylabel= 'loss')
axs[1].set_title('LSTM_Architecture')
axs[1].plot(list(range(1,len(history['accuracy'])+1 )), history['accuracy'])
axs[1].set(xlabel='No. of Epochs.',ylabel= 'Accuracy')

plt.show()

model = keras.models.load_model('/content/drive/MyDrive/Deep_learning/Group_9_Assignment6/results/LSTM_Architecure_32_64_128_256_model.h5')

predictions = model.predict(x_test)
conf_matrix=tf.math.confusion_matrix(y_test,predictions.argmax(axis=1))

accuracy = tf.metrics.Accuracy()(y_test, predictions.argmax(axis=1))

print("Confusion matrix:\n", conf_matrix.numpy())
print("Accuracy:", accuracy.numpy())