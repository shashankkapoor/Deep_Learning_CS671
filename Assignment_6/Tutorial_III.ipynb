{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3dhlU3amTSLe"
      },
      "source": [
        "# CS671 Deep Learning & Applications - Tutorial III\n",
        "\n",
        "# RNNs & LSTMs\n",
        "\n",
        "Date: 2 May 2023 | Instructor: Dr. Dileep A.D. | References: https://keras.io/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ7YVBOypQiE"
      },
      "source": [
        "### Basics RNNs/LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "VA8ZqprZppo8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, RNN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_thTlUs6r8Nv"
      },
      "source": [
        "* SimpleRNN is a fully-connected RNN layer, where the output of each time step is fed back into the input of the next time step, just like a traditional RNN. The activation function used in SimpleRNN is typically the hyperbolic tangent (tanh) function, although other activation functions can be used as well.\n",
        "\n",
        "* RNN is a more general-purpose RNN layer that allows for more flexibility in terms of the architecture of the RNN. Specifically, RNN allows you to specify the type of cell that you want to use for the RNN (e.g. LSTM or GRU), as well as the activation function that you want to use for the cell. This makes RNN more powerful and flexible than SimpleRNN, but also potentially more complex to use.\n",
        "\n",
        "* RNN provides a return_sequences argument, which allows you to control whether the layer should return the output of all time steps (i.e. a sequence) or just the output of the final time step. SimpleRNN, on the other hand, always returns the output of all time steps.\n",
        "\n",
        "* The SimpleRNN layer uses a hyperbolic tangent (tanh) activation function by default, while the base RNN layer uses a linear activation function by default. The activation function can be customized for both layers using the activation argument.\n",
        "\n",
        "* The output shape of the SimpleRNN layer is (batch_size, units), while the output shape of the base RNN layer is (batch_size, timesteps, units) where timesteps refers to the number of time steps in the input sequence.\n",
        "\n",
        "* Overall, the SimpleRNN layer is a simpler and more lightweight option for simple recurrent tasks, while the base RNN layer is a more general-purpose layer that can handle more complex recurrent tasks with variable sequence lengths."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MWEyluB5qJel"
      },
      "source": [
        "**SimpleRNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "YT1W6OSApQLr"
      },
      "outputs": [],
      "source": [
        "inputs = np.random.random([32, 10, 8]).astype(np.float32)\n",
        "simple_rnn = SimpleRNN(4)\n",
        "\n",
        "output = simple_rnn(inputs)  # The output has shape `[32, 4]`.\n",
        "\n",
        "simple_rnn = tf.keras.layers.SimpleRNN(\n",
        "    4, return_sequences=True, return_state=True)\n",
        "\n",
        "# whole_sequence_output has shape `[32, 10, 4]`.\n",
        "# final_state has shape `[32, 4]`.\n",
        "whole_sequence_output, final_state = simple_rnn(inputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nhjjlNl4qNbv"
      },
      "source": [
        "**LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "lKEkHXVxpQIs"
      },
      "outputs": [],
      "source": [
        "inputs = tf.random.normal([32, 10, 8])\n",
        "\n",
        "lstm = tf.keras.layers.LSTM(4)\n",
        "output = lstm(inputs)\n",
        "\n",
        "lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)\n",
        "whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D4HleeW1WHMG"
      },
      "source": [
        "### Handling Variable Length Inputs using Keras Masking\n",
        "\n",
        "*   Keras documentation on recurrent layers: https://keras.io/api/layers/recurrent_layers/simple_rnn/\n",
        "*   Keras documentation on masking: https://keras.io/api/layers/core_layers/masking/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vsikD_HlWej9"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Masking, SimpleRNN, Dense\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "SERYY_euYVxH"
      },
      "outputs": [],
      "source": [
        "X, seq_lengths = [], []\n",
        "for i in range(100):\n",
        "  l = np.random.randint(20, 50)\n",
        "  X.append(np.random.random((l, )))\n",
        "  seq_lengths.append(l)\n",
        "\n",
        "seq_lengths = np.array(seq_lengths)\n",
        "Y = np.random.randint(2, size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xwqK-pMNakwj"
      },
      "outputs": [],
      "source": [
        "max_len = np.max(seq_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv5hICFFXao2",
        "outputId": "da36e544-53d0-4aa7-fce4-ebec0313c461"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([48, 48, 30, 42, 28, 47, 28, 25, 39, 48]), 49)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq_lengths[:10], max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "PKcbgR7AZhwn"
      },
      "outputs": [],
      "source": [
        "# Set the mask value\n",
        "mask_value = 0 #np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "NDoSLdrvaT-Q"
      },
      "outputs": [],
      "source": [
        "X_padded, X_ = [], []\n",
        "for i in range(100):\n",
        "  pad = max_len - seq_lengths[i]\n",
        "  X_.extend(list(X[i]))\n",
        "  for i in range(pad):\n",
        "    X_.append(mask_value)\n",
        "  X_padded.append(X_)\n",
        "  X_ = []\n",
        "\n",
        "X_padded = np.array(X_padded)\n",
        "X_padded = np.expand_dims(X_padded, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVjJFvCtbs8a",
        "outputId": "f6b7bb03-4399-4849-e229-af4786f13a68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 49, 1)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "h44uFODQZhtm"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Masking(mask_value=mask_value, input_shape=(49, 1)))\n",
        "model.add(SimpleRNN(units=16))\n",
        "model.add(Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LAd6x4NWZ71H"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djFgMIZ4Z7yH",
        "outputId": "3275cf8e-73e9-4482-c77c-a1e0e0620636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "4/4 [==============================] - 3s 29ms/step - loss: 26.2859 - accuracy: 0.4300\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 25.8180 - accuracy: 0.4600\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 25.5480 - accuracy: 0.4600\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 25.3363 - accuracy: 0.4600\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 25.2217 - accuracy: 0.4700\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd0f8c28a90>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_padded, Y, batch_size=32, epochs=5, sample_weight=seq_lengths)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CPUCXPT_lSSm"
      },
      "source": [
        "### Handling Variable Length Input Using Pytorch\n",
        "\n",
        "Using: `pack_padded_sequence` and `pad_packed_sequence`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Pg02s2oNSP"
      },
      "source": [
        "PyTorch documentation on pack_padded_sequence() and pad_packed_sequence(): \n",
        "\n",
        "*   https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "\n",
        "*   https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html\n",
        "\n",
        "PyTorch tutorial on sequence-to-sequence modeling with attention:\n",
        "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8er-6N7n9GH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "D4HleeW1WHMG",
        "CPUCXPT_lSSm"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
