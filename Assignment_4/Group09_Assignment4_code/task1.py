# -*- coding: utf-8 -*-
"""Task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1osC3Zz49JZiskjjb3f7YZiKrwLnctfpi
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pickle
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam, SGD, Adagrad, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
from sklearn.decomposition import PCA

"""## Dataset"""

# def Dataset(path):
#     data = []
#     label= []
#     for subdir, dirs, filenames in os.walk(path):
#         for filename in filenames:
#             img=np.array(Image.open(os.path.join(subdir, filename)))
#             data.append(img)
#             label.append(float(subdir[-1]))
#     return data,label

# train_directory="/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/train"
# x_train,y_train=Dataset(train_directory)

# val_directory="/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/val"
# x_val,y_val=Dataset(val_directory)

# test_directory="/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/test"
# x_test,y_test=Dataset(test_directory)

# x_train = np.array(x_train).astype("float32") / 255.0
# x_test = np.array(x_test).astype("float32") / 255.0
# x_val =np.array(x_val).astype("float32") / 255.0

# x_train=x_train.tolist()
# x_test=x_test.tolist()
# x_val=x_val.tolist()

# with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/x_train.pkl", 'wb') as f:
#     pickle.dump(x_train, f)

# with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/y_train.pkl", 'wb') as f:
#     pickle.dump(y_train, f)

# with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/x_test.pkl", 'wb') as f:
#     pickle.dump(x_test, f)

# with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/y_test.pkl", 'wb') as f:
#     pickle.dump(y_test, f)

# with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/x_val.pkl", 'wb') as f:
#     pickle.dump(x_val, f)

# with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/y_val.pkl", 'wb') as f:
#     pickle.dump(y_val, f)

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/x_train.pkl', 'rb') as f:
    x_train = pickle.load(f)
with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/y_train.pkl', 'rb') as f:
    y_train = pickle.load(f)

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/x_test.pkl', 'rb') as f:
    x_test = pickle.load(f)
with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/y_test.pkl', 'rb') as f:
    y_test = pickle.load(f)

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/x_val.pkl', 'rb') as f:
    x_val = pickle.load(f)
with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/y_val.pkl', 'rb') as f:
    y_val = pickle.load(f)

def relabel(data):
  class_labels = np.array(data)
  label_map = {label: i for i, label in enumerate(np.unique(class_labels))}
  integer_labels = np.array([label_map[label] for label in class_labels])
  print(label_map)
  return integer_labels.tolist()

y_train=relabel(y_train)
y_test=relabel(y_test)
y_val=relabel(y_val)

"""## PCA"""

x_train = np.array(x_train).reshape(-1, 784)
x_test = np.array(x_test).reshape(-1, 784)
x_val = np.array(x_val).reshape(-1, 784)

"""#### n_components =32"""

k=32
pca = PCA(n_components=k)
x_train = pca.fit_transform(x_train).tolist()
x_test = pca.transform(x_test).tolist()
x_val = pca.transform(x_val).tolist()
print(np.array(x_train).shape)

"""#### n_components =64"""

k=64
pca = PCA(n_components=k)
x_train = pca.fit_transform(x_train).tolist()
x_test = pca.transform(x_test).tolist()
x_val = pca.transform(x_val).tolist()
print(np.array(x_train).shape)

"""#### n_components =128"""

k=128
pca = PCA(n_components=k)
x_train = pca.fit_transform(x_train).tolist()
x_test = pca.transform(x_test).tolist()
x_val = pca.transform(x_val).tolist()
print(np.array(x_train).shape)

"""#### n_components =256"""

k=256
pca = PCA(n_components=k)
x_train = pca.fit_transform(x_train).tolist()
x_test = pca.transform(x_test).tolist()
x_val = pca.transform(x_val).tolist()
print(np.array(x_train).shape)

"""#### Parameters"""

#num_classes = 10 
# num_features = 784
# learning_rate = 0.001

# # batch_size = 1
# # #display_step = 100
# #model 1
# n_hidden_1 = 128  
# n_hidden_2 = 64  
# n_hidden_3 = 32

# #model 2
# n_hidden_1 = 256 
# n_hidden_2 = 128 
# n_hidden_3 = 64

#model 3
n_hidden_1 = 512  
n_hidden_2 = 256  
n_hidden_3 = 128

"""## Model 1"""

initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=100)
early_stopping = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.0001)

model = keras.Sequential([
    
        layers.Input(shape=(k,),name="Input_layer"),
        layers.Dense(n_hidden_1, activation="tanh", name="Hidden_layer1"),
        layers.Dense(n_hidden_2, activation="tanh", name="Hidden_layer2"),
        layers.Dense(n_hidden_3, activation="tanh", name="Hidden_layer3"),
        layers.Dense(5, activation="softmax", name="output"),
        ])
model.summary()

Adam_optimizer=Adam(learning_rate=0.001, beta_1=0.9,beta_2=0.999, epsilon=1e-08, name="Adam")

model.compile(optimizer=Adam_optimizer, loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=1, epochs=500000, validation_data=(x_val, y_val),verbose=1,shuffle=True,callbacks=[early_stopping], validation_split=0.0, validation_batch_size=None)

test_loss, test_acc = model.evaluate(x_test, y_test,batch_size=None, verbose=1,callbacks=None)
print(f"Optimizer {Adam_optimizer.__class__.__name__}: test accuracy={test_acc}, epochs={len(history.history['accuracy'])}")

with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/PCA_"+str(k)+"/Adam_"+str(k)+"components_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

model.save(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/PCA_"+str(k)+"/Adam_"+str(k)+"components_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_model.h5")

"""#### Results"""

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/PCA_256/Adam_256components_512_256_128_history.pkl', 'rb') as f:
    history = pickle.load(f)

print("Total No. of Epochs to converge "+ str(len(history['accuracy']))+"\n")
print("Training Accuracy "+ str(history['accuracy'][-1]))
print("Validation Accuracy "+ str(history['val_accuracy'][-1]))

model = keras.models.load_model('/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/PCA_256/Adam_256components_256_128_64_model.h5')
#model.reset_states()
predictions = model.predict(x_test)
conf_matrix=tf.math.confusion_matrix(y_test,predictions.argmax(axis=1))

accuracy = tf.metrics.Accuracy()(y_test, predictions.argmax(axis=1))

# Print the metrics
print("Confusion matrix:\n", conf_matrix.numpy())
print("Accuracy:", accuracy.numpy())