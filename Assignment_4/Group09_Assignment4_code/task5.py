# -*- coding: utf-8 -*-
"""Task5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1866q4jHOdE5OZ6ExuMXLt1MSbG5XsHN5
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pickle
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from keras.models import Model
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam, SGD, Adagrad, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard

"""## Dataset"""

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/x_train.pkl', 'rb') as f:
    x_train = pickle.load(f)
with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/y_train.pkl', 'rb') as f:
    y_train = pickle.load(f)

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/x_test.pkl', 'rb') as f:
    x_test = pickle.load(f)
with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/y_test.pkl', 'rb') as f:
    y_test = pickle.load(f)

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/x_val.pkl', 'rb') as f:
    x_val = pickle.load(f)
with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/y_val.pkl', 'rb') as f:
    y_val = pickle.load(f)

def relabel(data):
  class_labels = np.array(data)
  label_map = {label: i for i, label in enumerate(np.unique(class_labels))}
  integer_labels = np.array([label_map[label] for label in class_labels])
  print(label_map)
  return integer_labels.tolist()

y_train=relabel(y_train)
y_test=relabel(y_test)
y_val=relabel(y_val)

x_train = np.array(x_train).reshape(-1, 784)
x_test = np.array(x_test).reshape(-1, 784)
x_val = np.array(x_val).reshape(-1, 784)

"""#### Adding 20% noise"""

noise_factor_20 = 0.2
x_train_noisy_20 = x_train + noise_factor_20 * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)

"""#### Adding 40% noise"""

noise_factor_40 = 0.4
x_train_noisy_40 = x_train + noise_factor_40 * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)

"""#### 1-layer Autoencoders"""

initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=100)
early_stopping = EarlyStopping(monitor='loss', patience=5, min_delta=0.0001)

input_size=28*28

bottleneck_size=64 

model = keras.Sequential([
    layers.Input(shape=(input_size,),name="Input_layer"),
    layers.Dense(bottleneck_size, activation='tanh',name="Bottleneck_layer"),
    layers.Dense(input_size, activation='linear',name="output")
])
model.summary()

Adam_optimizer=Adam(learning_rate=0.001, beta_1=0.9,beta_2=0.999, epsilon=1e-08, name="Adam")

model.compile(optimizer=Adam_optimizer, loss='mean_squared_error', metrics=['accuracy'])

history = model.fit(x_train_noisy_40, x_train, batch_size=1, epochs=500000, verbose=1, shuffle=True, callbacks=[early_stopping], validation_split=0.0)

with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/40_Denoising_Autoencoder/"+str(bottleneck_size)+"_1-layer_Autoencoder_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

model.save(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/40_Denoising_Autoencoder/"+str(bottleneck_size)+"_1-layer_Autoencoder_model.h5")

"""#### Results"""

x_train = x_train.tolist()
x_test = x_test.tolist()
x_val = x_val.tolist()

autoencoder =  keras.models.load_model('/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/40_Denoising_Autoencoder/64_1-layer_Autoencoder_model.h5')

loss,accuracy= autoencoder.evaluate(x_train, x_train, batch_size=None, verbose=1, callbacks=None)
print(f"Average reconstruction training error: {loss:.4f}")

loss,accuracy= autoencoder.evaluate(x_val, x_val, batch_size=None, verbose=1, callbacks=None)
print(f"Average reconstruction validation error: {loss:.4f}")

loss,accuracy= autoencoder.evaluate(x_test, x_test, batch_size=None, verbose=1, callbacks=None)
print(f"Average reconstruction testing error: {loss:.4f}")

def Dataset(path):
    data = []
    #label=[]
    for subdir, dirs, filenames in os.walk(path):
        for filename in filenames:
            img=np.array(Image.open(os.path.join(subdir, filename)))
            data.append(img)
            #label.append(float(subdir[-1]))
    return data #,label

train_directory="/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/Test"
x_train = Dataset(train_directory)
x_train = np.array(x_train).astype("float32") / 255.0
x_train = x_train.reshape(-1, 784).tolist()

model =  keras.models.load_model('/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/40_Denoising_Autoencoder/64_1-layer_Autoencoder_model.h5')
reconstructed_train_data = model.predict(x_train)

x_train=np.array(x_train).reshape(5,28,28)
reconstructed_train_data=np.array(reconstructed_train_data).reshape(-1,28,28)

fig, axs = plt.subplots(1, 5, figsize=(12, 12))


axs[0].imshow(x_train[0,:,:])
axs[1].imshow(x_train[1,:,:])
axs[2].set_title('Testing Image')
axs[2].imshow(x_train[2,:,:])
axs[3].imshow(x_train[3,:,:])
axs[4].imshow(x_train[4,:,:])
plt.show()

fig, axs = plt.subplots(1, 5, figsize=(12,12))


axs[0].imshow(reconstructed_train_data[0,:,:])
axs[1].imshow(reconstructed_train_data[1,:,:])
axs[2].set_title('Reconstructed Image')
axs[2].imshow(reconstructed_train_data[2,:,:])
axs[3].imshow(reconstructed_train_data[3,:,:])
axs[4].imshow(reconstructed_train_data[4,:,:])
plt.show()

"""#### Encode """

x_train = x_train.tolist()
x_test = x_test.tolist()
x_val = x_val.tolist()

autoencoder = keras.models.load_model('/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/40_Denoising_Autoencoder/64_1-layer_Autoencoder_model.h5')

encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('Bottleneck_layer').output)
encoder.summary()

compressed_x_train= encoder.predict(x_train)
compressed_x_test= encoder.predict(x_test)
compressed_x_val= encoder.predict(x_val)

compressed_x_train=compressed_x_train.tolist()
compressed_x_test=compressed_x_test.tolist()
compressed_x_val=compressed_x_val.tolist()

"""#### FCNN Architecture"""

# #model 1
# n_hidden_1 = 128  
# n_hidden_2 = 64  
# n_hidden_3 = 32

# #model 2
# n_hidden_1 = 256  
# n_hidden_2 = 128  
# n_hidden_3 = 64

#model 3
n_hidden_1 = 512  
n_hidden_2 = 256
n_hidden_3 = 128

initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=100)
early_stopping = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.0001)

bottleneck_size=64

model = keras.Sequential([
        layers.Input(shape=(bottleneck_size, ),name="Input_layer"),
        layers.Dense(n_hidden_1, activation="tanh", name="Hidden_layer1"),
        layers.Dense(n_hidden_2, activation="tanh", name="Hidden_layer2"),
        layers.Dense(n_hidden_3, activation="tanh", name="Hidden_layer3"),
        layers.Dense(5, activation="softmax", name="output"),
        ])
model.summary()

Adam_optimizer=Adam(learning_rate=0.001, beta_1=0.9,beta_2=0.999, epsilon=1e-08, name="Adam")

model.compile(optimizer=Adam_optimizer, loss= "sparse_categorical_crossentropy", metrics=['accuracy'])

history = model.fit(compressed_x_train, y_train, batch_size=1, epochs=500000, validation_data=(compressed_x_val, y_val), verbose=1, shuffle=True, callbacks=[early_stopping], validation_split=0.0, validation_batch_size=None)

with open(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/20_Classification_Architecture/"+str(bottleneck_size)+"components_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

model.save(f"/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/20_Classification_Architecture/"+str(bottleneck_size)+"components_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_model.h5")

"""#### Result"""

model = keras.models.load_model('/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/40_Classification_Architecture/64components_512_256_128_model.h5')
test_loss, test_acc = model.evaluate(compressed_x_test, y_test,batch_size=None, verbose=1,callbacks=None)

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assignment4/40_Classification_Architecture/64components_512_256_128_history.pkl', 'rb') as f:
    history = pickle.load(f)

print("Total No. of Epochs to converge "+ str(len(history['accuracy']))+"\n")
print("Training Accuracy "+ str(history['accuracy'][-1]))
print("Validation Accuracy "+ str(history['val_accuracy'][-1]))