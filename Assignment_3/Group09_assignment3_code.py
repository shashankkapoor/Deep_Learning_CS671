# -*- coding: utf-8 -*-
"""Group09_Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/152jzRU0AG4xyvfQAdQClfhbjYZ68muMn

#### Mounting Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""#### Importing necessary libraries"""

import os
import pickle
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam, SGD, Adagrad, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard

"""#### Dataset"""

def Dataset(path):
    data = []
    label= []
    for subdir, dirs, filenames in os.walk(path):
        for filename in filenames:
            img=np.array(Image.open(os.path.join(subdir, filename)))
            data.append(img)
            label.append(float(subdir[-1]))
    return data,label

train_directory="/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/train"
x_train,y_train=Dataset(train_directory)

val_directory="/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/val"
x_val,y_val=Dataset(val_directory)

test_directory="/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/test"
x_test,y_test=Dataset(test_directory)

x_train = np.array(x_train).astype("float32") / 255.0
x_test = np.array(x_test).astype("float32") / 255.0
x_val =np.array(x_val).astype("float32") / 255.0

x_train=x_train.tolist()
x_test=x_test.tolist()
x_val=x_val.tolist()

"""#### parameters"""

#num_classes = 10 
# num_features = 784
# learning_rate = 0.001

# # batch_size = 1
# # #display_step = 100

n_hidden_1 = 512 
n_hidden_2 = 256 
n_hidden_3 = 128
n_hidden_4 = 64
n_hidden_5 = 32

"""#### SGD Model"""

initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=100)
early_stopping = EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001)

model = keras.Sequential([
        layers.Flatten(input_shape=(28, 28),name="Input_layer"),
        layers.Dense(n_hidden_1, activation="sigmoid", name="Hidden_layer1",kernel_initializer=initializer),
        layers.Dense(n_hidden_2, activation="sigmoid", name="Hidden_layer2",kernel_initializer=initializer),
        layers.Dense(n_hidden_3, activation="sigmoid", name="Hidden_layer3",kernel_initializer=initializer),
        layers.Dense(n_hidden_4, activation="sigmoid", name="Hidden_layer4",kernel_initializer=initializer),
        layers.Dense(n_hidden_5, activation="sigmoid", name="Hidden_layer5",kernel_initializer=initializer),
        layers.Dense(10, activation="softmax", name="output",kernel_initializer=initializer),
        ])
model.summary()

sgd_optimizer=SGD(learning_rate=0.001,momentum=0,name='SGD')

model.compile(optimizer=sgd_optimizer, loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=1, epochs=500000, validation_data=(x_val, y_val), verbose=1, shuffle=True, callbacks=[early_stopping], validation_split=0.0, validation_batch_size=None)

test_loss, test_acc = model.evaluate(x_test, y_test, batch_size=None, verbose=1, callbacks=None)
print(f"Optimizer {sgd_optimizer.__class__.__name__}: test accuracy={test_acc}, epochs={len(history.history['accuracy'])}")

with open(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\SGD_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

model.save(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\SGD_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_model.h5")

#history.history['loss']

#history.history['accuracy']

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/128_64_32/SGD_128_64_32_history.pkl', 'rb') as f:
    history_sgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_sgd['accuracy'])+1 )),  history_sgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd['loss'])+1 )), history_sgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64/SGD_256_128_64_history.pkl', 'rb') as f:
    history_sgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(256,128,64)_Architecture')
axs.plot(list(range(1,len(history_sgd['accuracy'])+1 )),  history_sgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd['loss'])+1 )), history_sgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128/SGD_512_256_128_history.pkl', 'rb') as f:
    history_sgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128)_Architecture')
axs.plot(list(range(1,len(history_sgd['accuracy'])+1 )),  history_sgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd['loss'])+1 )), history_sgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64_32/SGD_256_128_64_history.pkl', 'rb') as f:
    history_sgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(256,128,64,32)_Architecture')
axs.plot(list(range(1,len(history_sgd['accuracy'])+1 )),  history_sgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd['loss'])+1 )), history_sgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64/SGD_512_256_128_64_history.pkl', 'rb') as f:
    history_sgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128,64)_Architecture')
axs.plot(list(range(1,len(history_sgd['accuracy'])+1 )),  history_sgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd['loss'])+1 )), history_sgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64_32/SGD_512_256_128_history.pkl', 'rb') as f:
    history_sgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128,64,32)_Architecture')
axs.plot(list(range(1,len(history_sgd['accuracy'])+1 )),  history_sgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd['loss'])+1 )), history_sgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd['accuracy']))+"\n")

"""#### VGD Model"""

initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=100)
early_stopping = EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001)

model = keras.Sequential([
        layers.Flatten(input_shape=(28, 28),name="Input_layer"),
        layers.Dense(n_hidden_1, activation="sigmoid", name="Hidden_layer1",kernel_initializer=initializer),
        layers.Dense(n_hidden_2, activation="sigmoid", name="Hidden_layer2",kernel_initializer=initializer),
        layers.Dense(n_hidden_3, activation="sigmoid", name="Hidden_layer3",kernel_initializer=initializer),
        layers.Dense(n_hidden_4, activation="sigmoid", name="Hidden_layer4",kernel_initializer=initializer),
        layers.Dense(n_hidden_5, activation="sigmoid", name="Hidden_layer5",kernel_initializer=initializer),
        layers.Dense(10, activation="softmax", name="output",kernel_initializer=initializer),
        ])
model.summary()

vgd_optimizer=SGD(learning_rate=0.001,momentum=0,name='VGD')
early_stopping = EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001)

model.compile(optimizer=vgd_optimizer, loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=len(x_train), epochs=500000, validation_data=(x_val, y_val),verbose=1,shuffle=True,callbacks=[early_stopping], validation_split=0.0, validation_batch_size=None)

test_loss, test_acc = model.evaluate(x_test, y_test, batch_size=None, verbose=1,callbacks=None)
print(f"Optimizer {vgd_optimizer.__class__.__name__}: test accuracy={test_acc}, epochs={len(history.history['accuracy'])}")

with open(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\VGD_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

# with open('history.pkl', 'rb') as f:
#     history = pickle.load(f)

model.save(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\VGD_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_model.h5")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/128_64_32/VGD_128_64_32_history.pkl', 'rb') as f:
    history_vgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_vgd['accuracy'])+1 )),  history_vgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_vgd['loss'])+1 )), history_vgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_vgd['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64/VGD_256_128_64_history.pkl', 'rb') as f:
    history_vgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_vgd['accuracy'])+1 )),  history_vgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_vgd['loss'])+1 )), history_vgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_vgd['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128/VGD_512_256_128_history.pkl', 'rb') as f:
    history_vgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128)_Architecture')
#axs.plot(list(range(1,len(history_vgd['accuracy'])+1 )),  history_vgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_vgd['loss'])+1 )), history_vgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_vgd['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64_32/VGD_256_128_64_history.pkl', 'rb') as f:
    history_vgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_vgd['accuracy'])+1 )),  history_vgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_vgd['loss'])+1 )), history_vgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_vgd['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64/VGD_512_256_128_64_history.pkl', 'rb') as f:
    history_vgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_vgd['accuracy'])+1 )),  history_vgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_vgd['loss'])+1 )), history_vgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_vgd['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64_32/VGD_512_256_128_history.pkl', 'rb') as f:
    history_vgd = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_vgd['accuracy'])+1 )),  history_vgd['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_vgd['loss'])+1 )), history_vgd['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_vgd['accuracy']))+"\n")

"""#### SGD with momentum  Model"""

initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=100)
early_stopping = EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001)

model = keras.Sequential([
        layers.Flatten(input_shape=(28, 28),name="Input_layer"),
        layers.Dense(n_hidden_1, activation="sigmoid", name="Hidden_layer1",kernel_initializer=initializer),
        layers.Dense(n_hidden_2, activation="sigmoid", name="Hidden_layer2",kernel_initializer=initializer),
        layers.Dense(n_hidden_3, activation="sigmoid", name="Hidden_layer3",kernel_initializer=initializer),
        layers.Dense(n_hidden_4, activation="sigmoid", name="Hidden_layer4",kernel_initializer=initializer),
        layers.Dense(n_hidden_5, activation="sigmoid", name="Hidden_layer5",kernel_initializer=initializer),
        layers.Dense(10, activation="softmax", name="output",kernel_initializer=initializer),
        ])
model.summary()

gdr_optimizer=SGD(learning_rate=0.001, momentum=0.9,name='Momentum_SGD')

model.compile(optimizer=gdr_optimizer, loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=1, epochs=500000, validation_data=(x_val, y_val),verbose=1,shuffle=True,callbacks=[early_stopping], validation_split=0.0, validation_batch_size=None)

test_loss, test_acc = model.evaluate(x_test, y_test,batch_size=None, verbose=1,callbacks=None)
print(f"Optimizer {gdr_optimizer.__class__.__name__}: test accuracy={test_acc}, epochs={len(history.history['accuracy'])}")

with open(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\MomentumSGD_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

# with open('history.pkl', 'rb') as f:
#     history = pickle.load(f)

model.save(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\MomentumSGD_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_model.h5")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/128_64_32/MomentumSGD_128_64_32_history.pkl', 'rb') as f:
    history_sgd_Moment = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_sgd_Moment['accuracy'])+1 )),  history_sgd_Moment['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd_Moment['loss'])+1 )), history_sgd_Moment['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd_Moment['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64/MomentumSGD_256_128_64_history.pkl', 'rb') as f:
    history_sgd_Moment = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_sgd_Moment['accuracy'])+1 )),  history_sgd_Moment['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd_Moment['loss'])+1 )), history_sgd_Moment['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd_Moment['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128/MomentumSGD_512_256_128_history.pkl', 'rb') as f:
    history_sgd_Moment = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128)_Architecture')
#axs.plot(list(range(1,len(history_sgd_Moment['accuracy'])+1 )),  history_sgd_Moment['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd_Moment['loss'])+1 )), history_sgd_Moment['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd_Moment['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64_32/MomentumSGD_256_128_64_history.pkl', 'rb') as f:
    history_sgd_Moment = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_sgd_Moment['accuracy'])+1 )),  history_sgd_Moment['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd_Moment['loss'])+1 )), history_sgd_Moment['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd_Moment['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64/MomentumSGD_512_256_128_64_history.pkl', 'rb') as f:
    history_sgd_Moment = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_sgd_Moment['accuracy'])+1 )),  history_sgd_Moment['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd_Moment['loss'])+1 )), history_sgd_Moment['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd_Moment['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64_32/MomentumSGD_512_256_128_history.pkl', 'rb') as f:
    history_sgd_Moment = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_sgd_Moment['accuracy'])+1 )),  history_sgd_Moment['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_sgd_Moment['loss'])+1 )), history_sgd_Moment['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_sgd_Moment['accuracy']))+"\n")

"""#### NAG Model"""

initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=100)
early_stopping = EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001)

model = keras.Sequential([
        layers.Flatten(input_shape=(28, 28),name="Input_layer"),
        layers.Dense(n_hidden_1, activation="sigmoid", name="Hidden_layer1",kernel_initializer=initializer),
        layers.Dense(n_hidden_2, activation="sigmoid", name="Hidden_layer2",kernel_initializer=initializer),
        layers.Dense(n_hidden_3, activation="sigmoid", name="Hidden_layer3",kernel_initializer=initializer),
        layers.Dense(n_hidden_4, activation="sigmoid", name="Hidden_layer4",kernel_initializer=initializer),
        layers.Dense(n_hidden_5, activation="sigmoid", name="Hidden_layer5",kernel_initializer=initializer),
        layers.Dense(10, activation="softmax", name="output",kernel_initializer=initializer),
        ])
model.summary()

nag_optimizer=SGD(learning_rate=0.001, momentum=0.9,nesterov=True, name='NAG')

model.compile(optimizer=nag_optimizer, loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=1, epochs=500000, validation_data=(x_val, y_val),verbose=1,shuffle=True, callbacks=[early_stopping], validation_split=0.0, validation_batch_size=None)

test_loss, test_acc = model.evaluate(x_test, y_test,batch_size=None, verbose=1,callbacks=None)
print(f"Optimizer {nag_optimizer.__class__.__name__}: test accuracy={test_acc}, epochs={len(history.history['accuracy'])}")

with open(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\NAG_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

# with open('history.pkl', 'rb') as f:
#     history = pickle.load(f)

model.save(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\NAG_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_model.h5")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/128_64_32/NAG_128_64_32_history.pkl', 'rb') as f:
    history_nag = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_nag['accuracy'])+1 )),  history_nag['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_nag['loss'])+1 )), history_nag['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_nag['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64/NAG_256_128_64_history.pkl', 'rb') as f:
    history_nag = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_nag['accuracy'])+1 )),  history_nag['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_nag['loss'])+1 )), history_nag['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_nag['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128/NAG_512_256_128_history.pkl', 'rb') as f:
    history_nag = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128)_Architecture')
#axs.plot(list(range(1,len(history_nag['accuracy'])+1 )),  history_nag['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_nag['loss'])+1 )), history_nag['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_nag['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64_32/NAG_256_128_64_history.pkl', 'rb') as f:
    history_nag = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_nag['accuracy'])+1 )),  history_nag['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_nag['loss'])+1 )), history_nag['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_nag['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64/NAG_512_256_128_64_history.pkl', 'rb') as f:
    history_nag = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_nag['accuracy'])+1 )),  history_nag['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_nag['loss'])+1 )), history_nag['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_nag['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64_32/NAG_512_256_128_history.pkl', 'rb') as f:
    history_nag = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('SGD_(512,256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_nag['accuracy'])+1 )),  history_nag['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_nag['loss'])+1 )), history_nag['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_nag['accuracy']))+"\n")

"""#### Adagrad Model"""

initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=100)
early_stopping = EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001)

model = keras.Sequential([
        layers.Flatten(input_shape=(28, 28),name="Input_layer"),
        layers.Dense(n_hidden_1, activation="sigmoid", name="Hidden_layer1",kernel_initializer=initializer),
        layers.Dense(n_hidden_2, activation="sigmoid", name="Hidden_layer2",kernel_initializer=initializer),
        layers.Dense(n_hidden_3, activation="sigmoid", name="Hidden_layer3",kernel_initializer=initializer),
        layers.Dense(n_hidden_4, activation="sigmoid", name="Hidden_layer4",kernel_initializer=initializer),
        layers.Dense(n_hidden_5, activation="sigmoid", name="Hidden_layer5",kernel_initializer=initializer),
        layers.Dense(10, activation="softmax", name="output",kernel_initializer=initializer),
        ])
model.summary()

Adagrad_optimizer=Adagrad(learning_rate=0.001, epsilon=1e-07, name="Adagrad")

model.compile(optimizer=Adagrad_optimizer, loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=1, epochs=500000, validation_data=(x_val, y_val),verbose=1,shuffle=True, callbacks=[early_stopping], validation_split=0.0, validation_batch_size=None)

test_loss, test_acc = model.evaluate(x_test, y_test,batch_size=None, verbose=1,callbacks=None)
print(f"Optimizer {Adagrad_optimizer.__class__.__name__}: test accuracy={test_acc}, epochs={len(history.history['accuracy'])}")

with open(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\Adagrad_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

# with open('history.pkl', 'rb') as f:
#     history = pickle.load(f)

model.save(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\Adagrad_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_model.h5")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/128_64_32/Adagrad_128_64_32_history.pkl', 'rb') as f:
    history_Adagrad = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adagrad_(128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_Adagrad['accuracy'])+1 )),  history_Adagrad['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adagrad['loss'])+1 )), history_Adagrad['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adagrad['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64/Adagrad_256_128_64_history.pkl', 'rb') as f:
    history_Adagrad = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adagrad_(256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_Adagrad['accuracy'])+1 )),  history_Adagrad['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adagrad['loss'])+1 )), history_Adagrad['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adagrad['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128/Adagrad_512_256_128_history.pkl', 'rb') as f:
    history_Adagrad = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adagrad_(512,256,128)_Architecture')
#axs.plot(list(range(1,len(history_Adagrad['accuracy'])+1 )),  history_Adagrad['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adagrad['loss'])+1 )), history_Adagrad['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adagrad['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64_32/Adagrad_256_128_64_history.pkl', 'rb') as f:
    history_Adagrad = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adagrad_(256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_Adagrad['accuracy'])+1 )),  history_Adagrad['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adagrad['loss'])+1 )), history_Adagrad['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adagrad['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64/Adagrad_512_256_128_64_history.pkl', 'rb') as f:
    history_Adagrad = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adagrad_(512,256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_Adagrad['accuracy'])+1 )),  history_Adagrad['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adagrad['loss'])+1 )), history_Adagrad['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adagrad['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64_32/Adagrad_512_256_128_history.pkl', 'rb') as f:
    history_Adagrad = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adagrad_(512,256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_Adagrad['accuracy'])+1 )),  history_Adagrad['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adagrad['loss'])+1 )), history_Adagrad['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adagrad['accuracy']))+"\n")

"""#### RMS Model"""

initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=100)
early_stopping = EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001)

model = keras.Sequential([
        layers.Flatten(input_shape=(28, 28),name="Input_layer"),
        layers.Dense(n_hidden_1, activation="sigmoid", name="Hidden_layer1",kernel_initializer=initializer),
        layers.Dense(n_hidden_2, activation="sigmoid", name="Hidden_layer2",kernel_initializer=initializer),
        layers.Dense(n_hidden_3, activation="sigmoid", name="Hidden_layer3",kernel_initializer=initializer),
        layers.Dense(n_hidden_4, activation="sigmoid", name="Hidden_layer4",kernel_initializer=initializer),
        layers.Dense(n_hidden_5, activation="sigmoid", name="Hidden_layer5",kernel_initializer=initializer),
        layers.Dense(10, activation="softmax", name="output",kernel_initializer=initializer),
        ])
model.summary()

rms_optimizer=RMSprop(learning_rate=0.001, rho=0.99, momentum=0.0, epsilon=1e-08, name="RMSProp")

model.compile(optimizer=rms_optimizer, loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=1, epochs=500000, validation_data=(x_val, y_val),verbose=1,shuffle=True,callbacks=[early_stopping], validation_split=0.0, validation_batch_size=None)

test_loss, test_acc = model.evaluate(x_test, y_test,batch_size=None, verbose=1,callbacks=None)
print(f"Optimizer {rms_optimizer.__class__.__name__}: test accuracy={test_acc}, epochs={len(history.history['accuracy'])}")

with open(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\RMS_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

# with open('history.pkl', 'rb') as f:
#     history = pickle.load(f)

model.save(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\RMS_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_model.h5")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/128_64_32/RMS_128_64_32_history.pkl', 'rb') as f:
    history_RMS = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('RMS_(128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_RMS['accuracy'])+1 )),  history_RMS['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_RMS['loss'])+1 )), history_RMS['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_RMS['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64/RMS_256_128_64_history.pkl', 'rb') as f:
    history_RMS = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('RMS_(256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_RMS['accuracy'])+1 )),  history_RMS['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_RMS['loss'])+1 )), history_RMS['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_RMS['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128/RMS_512_256_128_history.pkl', 'rb') as f:
    history_RMS = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('RMS_(512,256,128)_Architecture')
#axs.plot(list(range(1,len(history_RMS['accuracy'])+1 )),  history_RMS['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_RMS['loss'])+1 )), history_RMS['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_RMS['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64/RMS_512_256_128_64_history.pkl', 'rb') as f:
    history_RMS = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('RMS_(512,256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_RMS['accuracy'])+1 )),  history_RMS['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_RMS['loss'])+1 )), history_RMS['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_RMS['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64_32/RMS_256_128_64_history.pkl', 'rb') as f:
    history_RMS = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('RMS_(256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_RMS['accuracy'])+1 )),  history_RMS['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_RMS['loss'])+1 )), history_RMS['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_RMS['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64_32/RMS_512_256_128_history.pkl', 'rb') as f:
    history_RMS = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('RMS_(512,256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_RMS['accuracy'])+1 )),  history_RMS['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_RMS['loss'])+1 )), history_RMS['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_RMS['accuracy']))+"\n")

"""#### Adam model"""

initializer = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=100)
early_stopping = EarlyStopping(monitor='val_loss', patience=1, min_delta=0.0001)

model = keras.Sequential([
        layers.Flatten(input_shape=(28, 28),name="Input_layer"),
        layers.Dense(n_hidden_1, activation="sigmoid", name="Hidden_layer1",kernel_initializer=initializer),
        layers.Dense(n_hidden_2, activation="sigmoid", name="Hidden_layer2",kernel_initializer=initializer),
        layers.Dense(n_hidden_3, activation="sigmoid", name="Hidden_layer3",kernel_initializer=initializer),
        layers.Dense(n_hidden_4, activation="sigmoid", name="Hidden_layer4",kernel_initializer=initializer),
        layers.Dense(n_hidden_5, activation="sigmoid", name="Hidden_layer5",kernel_initializer=initializer),
        layers.Dense(10, activation="softmax", name="output",kernel_initializer=initializer),
        ])
model.summary()

Adam_optimizer=Adam(learning_rate=0.001, beta_1=0.9,beta_2=0.999, epsilon=1e-08, name="Adam")

model.compile(optimizer=Adam_optimizer, loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=1, epochs=500000, validation_data=(x_val, y_val),verbose=1,shuffle=True,callbacks=[early_stopping], validation_split=0.0, validation_batch_size=None)

test_loss, test_acc = model.evaluate(x_test, y_test,batch_size=None, verbose=1,callbacks=None)
print(f"Optimizer {Adam_optimizer.__class__.__name__}: test accuracy={test_acc}, epochs={len(history.history['accuracy'])}")

with open(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\Adam_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_history.pkl", 'wb') as f:
    pickle.dump(history.history, f)

# with open('history.pkl', 'rb') as f:
#     history = pickle.load(f)

model.save(f"C:\\Users\\User1\\Downloads\\Group_9_Assgnment3\\Group_9\\Adam_"+str(n_hidden_1)+"_"+str(n_hidden_2)+"_"+str(n_hidden_3)+"_model.h5")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/128_64_32/Adam_128_64_32_history.pkl', 'rb') as f:
    history_Adam = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adam_(128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_Adam['accuracy'])+1 )),  history_Adam['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adam['loss'])+1 )), history_Adam['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adam['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64/Adam_256_128_64_history.pkl', 'rb') as f:
    history_Adam = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adam_(256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_Adam['accuracy'])+1 )),  history_Adam['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adam['loss'])+1 )), history_Adam['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adam['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128/Adam_512_256_128_history.pkl', 'rb') as f:
    history_Adam = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adam_(512,256,128)_Architecture')
#axs.plot(list(range(1,len(history_Adam['accuracy'])+1 )),  history_Adam['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adam['loss'])+1 )), history_Adam['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adam['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/256_128_64_32/Adam_256_128_64_history.pkl', 'rb') as f:
    history_Adam = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adam_(256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_Adam['accuracy'])+1 )),  history_Adam['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adam['loss'])+1 )), history_Adam['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adam['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64/Adam_512_256_128_64_history.pkl', 'rb') as f:
    history_Adam = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adam_(512,256,128,64)_Architecture')
#axs.plot(list(range(1,len(history_Adam['accuracy'])+1 )),  history_Adam['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adam['loss'])+1 )), history_Adam['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adam['accuracy']))+"\n")

with open('/content/drive/MyDrive/Deep_learning/Group_9_Assgnment3/Group_9/Group_9/512_256_128_64_32/Adam_512_256_128_history.pkl', 'rb') as f:
    history_Adam = pickle.load(f)

fig, axs = plt.subplots(1,figsize=(7,1),sharex=True, sharey=False)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=1.5,wspace=0.4,hspace=0.4)

axs.set_title('Adam_(512,256,128,64,32)_Architecture')
#axs.plot(list(range(1,len(history_Adam['accuracy'])+1 )),  history_Adam['accuracy'] , color='blue', label='Accuracy')
axs.plot(list(range(1,len(history_Adam['loss'])+1 )), history_Adam['loss'], color='red', label='loss')
axs.legend()
axs.set(xlabel='No. of Epochs.')

print("Total No. of Epochs to converge "+ str(len(history_Adam['accuracy']))+"\n")

"""#### testing"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, optimizers
from tensorflow.keras.datasets import mnist

# Load the MNIST dataset and split into train, validation, and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# print(x_train.shape)
#y_true = tf.one_hot(y_train, depth=10)
print(len(y_train))
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0
x_val, y_val = x_train[-10000:], y_train[-10000:]
x_train, y_train = x_train[:-10000], y_train[:-10000]

initializer = keras.initializers.RandomUniform(minval=0, maxval=1, seed=7)

# Define the FCNN architecture
model = keras.Sequential([
    layers.Flatten(input_shape=(28, 28)),
    layers.Dense(128, activation='sigmoid',kernel_initializer=initializer),
    layers.Dense(64, activation='sigmoid',kernel_initializer=initializer),
    layers.Dense(10, activation='sigmoid',kernel_initializer=initializer)
])

for layer in model.layers:
    weights = layer.get_weights()
    print(weights)


# Choose the loss function
loss_fn = keras.losses.SparseCategoricalCrossentropy()

# Train the FCNN using different optimizers
optimizers = [
    optimizers.SGD(learning_rate=0.001,name='SGD'), #Normal Gradient Descent
    optimizers.SGD(learning_rate=0.001,name='SGD'), #Vanilla Gradient Descent
    optimizers.SGD(learning_rate=0.001, momentum=0.9,name='Momentum_SGD'), #Momentum Based
    optimizers.SGD(learning_rate=0.001, momentum=0.9,nesterov=True, name='NAG'), #NAG
    optimizers.Adagrad(learning_rate=0.001, epsilon=1e-8, name="Adagrad"), #AdaGrad
    optimizers.RMSprop(learning_rate=0.001, rho=0.99, momentum=0.0, epsilon=1e-08, name="RMSProp"), #RMSProp
    optimizers.Adam(learning_rate=0.001, name="Adam") #Adam
]

for optimizer in optimizers:
    # Compile the model with the chosen optimizer
    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])
    
    # Train the model
    history = model.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=(x_val, y_val))

    # Evaluate the model
    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
    print(f"Optimizer {optimizer.__class__.__name__}: test accuracy={test_acc}, epochs={len(history.history['accuracy'])}")